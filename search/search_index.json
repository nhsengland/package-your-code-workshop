{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Package Your Code Workshop","text":"<p>Welcome to the Package Your Code workshop! This workshop is designed to teach NHS data scientists and analysts how to package their code effectively for better collaboration, distribution, and reuse. In this workshop, you'll master the essential skills for creating professional, reusable Python packages. You'll learn how to manage dependencies, package your code using modern standards, and create comprehensive documentation.</p> <p>Pre-requisite Knowledge</p> <p>The workshop assumes that participants have a basic understanding of the following concepts:</p> Pre-requisite Description Python Knowledge of how to write and run Python code Git Basic command line usage and version control concepts GitHub Familiarity with repositories, Codespaces, and forking RAP Understanding of the core principles of Reproducible Analytical Pipelines (RAP), particularly the levels of RAP Virtual Environments &amp; Package Management Basic understanding of virtual environments and package management in Python (e.g., <code>pip</code> and <code>venv</code>)"},{"location":"#core-workshop-topics","title":"Core Workshop Topics","text":"<ul> <li> <p>Dependency Management</p> <ul> <li>Using <code>uv</code> for fast, reliable dependency management</li> <li>Organizing dependencies into production, development, and documentation categories</li> <li>Creating and managing virtual environments</li> </ul> </li> <li> <p>Packaging with pyproject.toml</p> <ul> <li>Understanding the modern Python packaging standard</li> <li>Configuring project metadata and dependencies</li> <li>Making your code installable and reusable</li> </ul> </li> <li> <p>Documentation with MkDocs</p> <ul> <li>Setting up professional documentation with MkDocs Material</li> <li>Using mkdocstrings for automatic API documentation</li> <li>Creating user-friendly guides and tutorials</li> </ul> </li> </ul>"},{"location":"#bonus-content","title":"Bonus Content","text":"<p>Work in Progress</p> <p>The bonus workshop are still a work in progress and have not been completed in time for the live delivery of the workshop.</p> <p>Explore these additional topics at your own pace:</p> <ul> <li>Cookiecutter Templates - Quickly scaffold new projects</li> <li>Pre-Commit Hooks - Automate code quality checks</li> <li>CI/CD with GitHub Actions - Automate testing and deployment</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to begin? Check out our Getting Started guide to set up your development environment and begin the workshop.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide will help you set up your development environment to participate in the Package Your Code workshop. Follow the steps below to ensure you can dive into the workshop content smoothly.</p>"},{"location":"getting_started/#fork-the-repository","title":"Fork the Repository","text":"<ol> <li>Navigate to the Package Your Code Workshop GitHub repository</li> <li>Click the \"Fork\" button in the top-right corner to create a copy of the repository under your GitHub account.</li> <li>Click \"Create fork\" to confirm.</li> </ol>"},{"location":"getting_started/#open-in-github-codespaces","title":"Open in GitHub Codespaces","text":"<ol> <li>In your forked repository, click the green \"Code\" button.</li> <li>Select the \"Codespaces\" tab.</li> <li>Click \"Create codespace on main\" to launch a new Codespace instance. This may take a few moments to set up.</li> <li>Once the Codespace is ready, you will be taken to a VS Code environment in your browser. The repository will be cloned, and the development environment will be configured automatically, give it a few moments to finish setting up.</li> </ol>"},{"location":"getting_started/#fire-up-the-mkdocs-server","title":"Fire Up the MkDocs Server","text":"<ol> <li>Open a new terminal in the Codespace (Terminal &gt; New Terminal).</li> <li>Run the following command to start the MkDocs development server: <code>mkdocs serve</code></li> <li>Once the server is running, you will see a message indicating that the site is being served at a URL. Click on the URL to open the documentation in a new browser tab. This will update automatically as you make changes to the documentation files (which will happen in the MKDocs section of the workshop).</li> </ol> <p>Stop the server</p> <p>To stop the MkDocs server, return to the terminal where it's running and press <code>Ctrl + C</code>.</p>"},{"location":"getting_started/#find-your-first-workshop","title":"Find your first workshop","text":"<p>Head over to the workshops section of the website to get started with the workshop content.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This section will contain the automatically generated API documentation for the NHS Hospital Analysis package.</p> <p>This section will be completed during the MkDocs workshop where you'll learn to set up automatic API documentation.</p>"},{"location":"workshop_presentation/","title":"Package Your Code Workshop \"Presentation\"","text":"<p>Welcome to the Package Your Code workshop! This series of workshops is designed to help you learn some of the best practices for packaging and distributing your Python code effectively. Whether you're a beginner or an experienced developer, these workshops will guide you through essential tools and techniques to enhance your coding projects.</p>"},{"location":"workshop_presentation/open_up_the_repository/","title":"Open up the repository","text":"<p>First things first, you will need to open up this website on your own machine.</p> <p>Follow this link: nhsengland.github.io/package-your-code-workshop/ </p> <p></p> <p>Then follow the Getting Started instructions to fork the repository and open it up in Codespaces.</p>"},{"location":"workshop_presentation/opinions_and_disclaimers/","title":"Opinions and Disclaimers","text":""},{"location":"workshop_presentation/opinions_and_disclaimers/#opinions","title":"Opinions","text":""},{"location":"workshop_presentation/opinions_and_disclaimers/#uv-is-the-tool-to-use-for-python-environment-management","title":"<code>uv</code> is the tool to use for Python Environment Management","text":"<p>Other tools are available. I just think <code>uv</code> is pretty neat. It does everything I need it to do, and it does it well. As you will see in the dependency management workshop, there should be interoperability between tools.</p>"},{"location":"workshop_presentation/opinions_and_disclaimers/#you-should-be-creating-mkdocs-sites-for-your-projects","title":"You should be creating MKDocs sites for your projects","text":"<p>I think MKDocs is a great way to create documentation for your projects. It is easy to use, and it integrates well with GitHub Pages. You can easily adapt other NHS England Mkdocs sites to use as a template for your own. However, you need to have a public repository for GitHub Pages to work.</p>"},{"location":"workshop_presentation/opinions_and_disclaimers/#you-should-be-creating-utility-packages-for-your-non-business-logic-code","title":"You should be creating utility packages for your non-business logic code","text":"<p>You should be packaging your code. If your pipeline is full of specific business logic or methods, you should look into what generic utility code you can extract into a package. This workshop will help you do that.</p>"},{"location":"workshop_presentation/opinions_and_disclaimers/#disclaimers","title":"Disclaimers","text":""},{"location":"workshop_presentation/opinions_and_disclaimers/#this-is-a-python-focused-workshop","title":"This is a Python focused workshop","text":"<p>Sorry R users. In NHS England Data Science team, we are a Python dominant team, so this is the language I am targeting this workshop at.If you are a R user, and you want help make a equivalent R workshop, please get in touch.</p>"},{"location":"workshop_presentation/opinions_and_disclaimers/#this-workshop-was-built-fast-using-ai","title":"This workshop was built fast using AI","text":"<p>I have used AI tools to help me build this workshop quickly. Its like having a very enthusiastic Junior Dev on a lot coffee.</p> <p>I have worked hard to check through every line of content that the AI has created and gone through the steps of the workshop myself (as have the facilitators). But, we might have missed things and there might be inaccuracies.</p> <p>If you run into problems or find any issues, please raise an issue on GitHub if you find any. This is an open-source workshop after all.</p>"},{"location":"workshop_presentation/opinions_and_disclaimers/#using-this-workshop-in-anger-on-real-platforms-and-projects-might-not-be-simple","title":"Using this workshop in anger on real platforms and projects might not be simple","text":"<p>We should be able to do this stuff on real projects and platforms. But, we know that real life is messy and it might not as simple to adopt these practices. Hopefully, you are able to take away principles and techniques from this workshop that will make it easier for you to do Gold RAP work.</p>"},{"location":"workshop_presentation/who_am_i/","title":"Who am I?","text":""},{"location":"workshop_presentation/who_am_i/#joe-wilson-data-scientist-at-nhs-england","title":"Joe Wilson - Data Scientist at NHS England","text":"<p>GitHub - josephwilson8-nhs</p> <ul> <li>Background in Aerospace Engineering and Data Science</li> <li>I code mainly in Python, but also in R and SQL (and LabVIEW from my engineering days)</li> <li>I have been working in the Reproducible Analytical Pipelines (RAP) space for a lot of my last four years at NHS England<ul> <li>Currently an embedded Data Scientist in the Direct Commissioning Data and Analytics team</li> </ul> </li> <li>I am the Tech Lead for Gold RAP Delivery<ul> <li>This means our pipelines are built to a high standard, using best practices, and can be seen as a reusable, deployable product.</li> <li>My mission: To make that easy, default behaviour for everyone</li> </ul> </li> </ul> <p>In the live workshop, I am joined by:</p> <ul> <li>Jen Struthers - jenniferstruthers1-nhs</li> <li>Warren Davies - warren-davies4</li> </ul>"},{"location":"workshop_presentation/workshop_context/","title":"Workshop Context - Why is any of this important?","text":""},{"location":"workshop_presentation/workshop_context/#i-want-to-make-gold-rap-easy-and-the-default","title":"I want to make Gold RAP easy and the default","text":"<p>It can be a lot easier to build Gold RAP pipelines if you are doing it from the start and/or it is baked into your workflow and the project plan.</p> <p>I want to make sure you know how you can do this stuff easily and effectively.</p> <p>I want us to build tools and templates to make it easy for you to do this stuff, so it becomes the default; you can be lazy and still do Gold RAP, allowing you focus on doing great Data Science. However, we need to do the learning and the work to develop these skills and find the lazy desired paths first.</p> <p>I hope this workshop helps you start to do that and you can help figure out solutions to to how we make Gold RAP the easy, lazy default.</p>"},{"location":"workshops/","title":"Workshop Overview","text":"<p>Lets dive into these workshops! Each workshop will take</p> <p>Learning Objectives</p> <p>By the end of these workshops, you'll be able to:</p> <ul> <li>Manage Python dependencies professionally</li> <li>Package your code for easy distribution and reuse</li> <li>Create comprehensive documentation that your users will love</li> <li>Set up automated quality checks and deployment pipelines</li> <li>Apply RAP principles to your data science projects</li> </ul>"},{"location":"workshops/#workshop-path","title":"Workshop Path","text":""},{"location":"workshops/#core-workshops-live-session","title":"Core Workshops (Live Session)","text":"<p>Complete these workshops in order during the live session:</p>"},{"location":"workshops/#dependency-management","title":"Dependency Management","text":"<ul> <li>Master modern Python package management with <code>uv</code></li> <li>Organize dependencies by purpose (production, development, docs)</li> <li>Create reproducible environments</li> </ul>"},{"location":"workshops/#packaging-with-pyprojecttoml","title":"Packaging with pyproject.toml","text":"<ul> <li>Configure project metadata and dependencies</li> <li>Make your code installable and reusable</li> <li>Follow modern Python packaging standards</li> </ul>"},{"location":"workshops/#documentation-with-mkdocs","title":"Documentation with MkDocs","text":"<ul> <li>Create professional documentation websites</li> <li>Automatically generate API documentation</li> <li>Deploy documentation to GitHub Pages</li> </ul>"},{"location":"workshops/#bonus-workshops-self-paced","title":"Bonus Workshops (Self-Paced)","text":"<p>Explore these advanced topics at your own pace:</p>"},{"location":"workshops/#cookiecutter-templates","title":"Cookiecutter Templates","text":"<ul> <li>Create reusable project templates</li> <li>Standardise team workflows</li> <li>Rapid project scaffolding</li> </ul>"},{"location":"workshops/#pre-commit-hooks","title":"Pre-Commit Hooks","text":"<ul> <li>Automate code quality checks</li> <li>Prevent common mistakes</li> <li>Enforce coding standards</li> </ul>"},{"location":"workshops/#cicd-with-github-actions","title":"CI/CD with GitHub Actions","text":"<ul> <li>Automate testing and deployment</li> <li>Build and publish packages</li> <li>Continuous integration best practices</li> </ul> <p>Getting Help</p> <p>During the workshops:</p> <ul> <li>Ask questions - Don't hesitate to ask for clarification</li> <li>Discussion time - Share your experiences and learn from others</li> <li>Stuck on something? - The facilitators are here to help</li> </ul> <p>Outside of the workshops, don't hesitate to open an issue in the repository if you encounter any problems or have suggestions for improvement.</p>"},{"location":"workshops/cookiecutter_templates/","title":"Cookiecutter Data Science: Professional Project Templates","text":"<p>Bonus Workshop - Self-Paced</p> <p>This is an optional, self-paced workshop. You can complete it at your own speed and refer back to it as needed.</p> <p>Learn how to create standardised, professional data science projects using Cookiecutter Data Science (CCDS), a proven template used by data scientists worldwide.</p> <p>Learning Objectives</p> <ul> <li>Understand the importance of standardized project structure in data science</li> <li>Install and use Cookiecutter to generate professional project templates</li> <li>Create a new data science project using the CCDS template</li> <li>Explore and customize the generated project structure</li> <li>Apply best practices for reproducible data science workflows</li> </ul> <p>Why This Matters for RAP</p> <p>Standardized project structure is fundamental to Silver RAP and essential for Gold RAP. Using proven templates like CCDS ensures your projects follow industry best practices from day one, making them more maintainable, collaborative, and reproducible.</p>"},{"location":"workshops/cookiecutter_templates/#what-is-cookiecutter-data-science","title":"What is Cookiecutter Data Science?","text":"<p>Cookiecutter Data Science (CCDS) is a standardized project template for data science projects, developed by DrivenData and used by thousands of data scientists worldwide.</p> <p>From the CCDS Team</p> <p>\"A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.\" - Cookiecutter Data Science</p>"},{"location":"workshops/cookiecutter_templates/#why-use-ccds","title":"Why Use CCDS?","text":"<ul> <li>Consistent Structure: Every project follows the same layout, making it easy for team members to navigate and contribute.</li> <li>Data Science Focused: Specifically designed for data science workflows with dedicated folders for data, notebooks, models, and reports.</li> <li>Reproducible: Includes configuration for environment management, dependency tracking, and documentation.</li> <li>Battle Tested: Used by multiple users for their production data science work, with continual improvements based on real-world feedback. </li> <li>Team Collaboration: New team members can quickly understand and contribute to any CCDS project.</li> </ul>"},{"location":"workshops/cookiecutter_templates/#task-1-understanding-project-structure-problems","title":"Task 1: Understanding Project Structure Problems","text":"<p>Before we dive into CCDS, let's understand why standardized project structure matters.</p>"},{"location":"workshops/cookiecutter_templates/#11-common-data-science-project-pitfalls","title":"1.1 Common Data Science Project Pitfalls","text":"<p>Without a standard structure, data science projects often suffer from:</p> <pre><code>my_analysis/\n\u251c\u2500\u2500 analysis.ipynb\n\u251c\u2500\u2500 data.csv\n\u251c\u2500\u2500 data_cleaned.csv\n\u251c\u2500\u2500 final_analysis.ipynb\n\u251c\u2500\u2500 final_analysis_v2.ipynb\n\u251c\u2500\u2500 final_analysis_FINAL.ipynb\n\u251c\u2500\u2500 model.pkl\n\u251c\u2500\u2500 plot1.png\n\u251c\u2500\u2500 plot2.png\n\u2514\u2500\u2500 README.txt\n</code></pre> <p>Problems with this approach:</p> <ul> <li>Hard to navigate - No clear organization</li> <li>Not reproducible - Unclear which files are inputs vs outputs</li> <li>Poor collaboration - Team members can't find what they need</li> <li>Doesn't scale - Becomes unwieldy as projects grow</li> <li>RAP non-compliant - Doesn't meet professional standards</li> </ul> <p>Note</p> <p>Typically, the projects aren't this bad and the point is exaggerated for effect. However, having logical standardised structures from the start of a project can help ensure consistency and professionalism as the project evolves.</p>"},{"location":"workshops/cookiecutter_templates/#12-the-ccds-solution","title":"1.2 The CCDS Solution","text":"<p>CCDS provides a logical, standardized structure that addresses these problems:</p> <pre><code>example/\n\u251c\u2500\u2500 LICENSE            # (1)!\n\u251c\u2500\u2500 Makefile           # (2)!\n\u251c\u2500\u2500 README.md          # (3)!\n\u251c\u2500\u2500 data               # (4)!\n\u2502   \u251c\u2500\u2500 external\n\u2502   \u251c\u2500\u2500 interim\n\u2502   \u251c\u2500\u2500 processed\n\u2502   \u2514\u2500\u2500 raw\n\u251c\u2500\u2500 docs               # (5)!\n\u251c\u2500\u2500 example # (6)!\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 dataset.py\n\u2502   \u251c\u2500\u2500 features.py\n\u2502   \u251c\u2500\u2500 modeling\n\u2502   \u2514\u2500\u2500 plots.py\n\u251c\u2500\u2500 models             # (7)!\n\u251c\u2500\u2500 notebooks          # (8)!\n\u251c\u2500\u2500 pyproject.toml     # (9)!\n\u251c\u2500\u2500 references         # (10)!\n\u251c\u2500\u2500 reports            # (11)!\n\u2502   \u2514\u2500\u2500 figures\n\u2514\u2500\u2500 tests              # (12)!\n</code></pre> <ol> <li>Open-source license</li> <li>Automated commands like <code>make data</code> or <code>make train</code></li> <li>Top-level project documentation</li> <li>Data directory with clear pipeline: raw \u2192 interim \u2192 processed \u2192 external</li> <li>MkDocs documentation project</li> <li>Source code package (named after your project)</li> <li>Trained models and predictions</li> <li>Jupyter notebooks for analysis</li> <li>Modern Python project configuration</li> <li>Data dictionaries and manuals</li> <li>Generated reports and figures</li> <li>Unit tests for your code</li> </ol> <p>Benefits of This Structure</p> <ul> <li>Clear data flow - From raw \u2192 interim \u2192 processed</li> <li>Organized code - Separate modules for different tasks</li> <li>Report ready - Dedicated space for outputs</li> <li>Team friendly - Anyone can navigate and contribute</li> <li>RAP compliant - Meets professional reproducibility standards</li> </ul>"},{"location":"workshops/cookiecutter_templates/#task-2-installing-ccds","title":"Task 2: Installing CCDS","text":"<p>Let's get CCDS (Cookiecutter Data Science) set up so we can generate professional project templates.</p>"},{"location":"workshops/cookiecutter_templates/#21-install-ccds","title":"2.1 Install CCDS","text":"<p>CCDS is distributed as a Python package called <code>cookiecutter-data-science</code>. Let's install it:</p> With UV (If you've done Dependency Management)With pip (Traditional approach)With pipx (Recommended by CCDS) <pre><code># Install cookiecutter-data-science globally using UV\nuv tool install cookiecutter-data-science\n\n# Verify installation\nuv tool run ccds --version\n</code></pre> <p>UV Tool Installation</p> <p>Using <code>uv tool install cookiecutter-data-science</code> installs the CCDS package globally and isolated from your projects. This is perfect for tools you want to use across multiple projects.</p> <pre><code># Install cookiecutter-data-science globally\npip install --user cookiecutter-data-science\n\n# Verify installation\nccds --version\n</code></pre> <pre><code># Install cookiecutter-data-science with pipx\npipx install cookiecutter-data-science\n\n# Verify installation\nccds --version\n</code></pre> <p>pipx Installation</p> <p>The official CCDS documentation recommends using pipx for cross-project utility applications like CCDS.</p>"},{"location":"workshops/cookiecutter_templates/#22-verify-installation","title":"2.2 Verify Installation","text":"<p>Test that CCDS is working correctly:</p> With UVWith pip <pre><code># Test CCDS\nuv tool run ccds --help\n</code></pre> <pre><code># Test CCDS\nccds --help\n</code></pre> <p>Expected Output</p> <p>You should see the CCDS help text with available commands and options for creating data science projects.</p>"},{"location":"workshops/cookiecutter_templates/#task-3-creating-your-first-ccds-project","title":"Task 3: Creating Your First CCDS Project","text":"<p>Now let's use CCDS to create a professional data science project following the standardized template.</p>"},{"location":"workshops/cookiecutter_templates/#31-generate-a-new-project","title":"3.1 Generate a New Project","text":"<p>We'll create a project for analyzing NHS GP appointment data (similar to our workshop example):</p> With UVWith pipWith pipx <pre><code># Create a new CCDS project\nuv tool run ccds https://github.com/drivendataorg/cookiecutter-data-science\n</code></pre> <pre><code># Create a new CCDS project\nccds https://github.com/drivendataorg/cookiecutter-data-science\n</code></pre> <pre><code># Create a new CCDS project\nccds https://github.com/drivendataorg/cookiecutter-data-science\n</code></pre> <p>CCDS Command</p> <p>The <code>ccds</code> command now requires the full GitHub URL to the latest Cookiecutter Data Science template. This ensures you get the most up-to-date version with all the latest features and options.</p>"},{"location":"workshops/cookiecutter_templates/#32-configure-your-project","title":"3.2 Configure Your Project","text":"<p>CCDS will prompt you for project details. Here's an example configuration for an NHS data science project:</p> <pre><code>$ ccds\nYou've downloaded /home/jowi60/.cookiecutters/cookiecutter-data-science before. Is it okay to delete and re-download it? [y/n] (y):\nproject_name (project_name): example_nhs_project # (1)!\nrepo_name (example_nhs_project): # (2)!\nmodule_name (example_nhs_project): # (3)!\nauthor_name (Your name (or your organization/company/team)): NHS Data Science Team # (4)!\ndescription (A short description of the project.): This is simply an example of using CCDS to create a project # (5)!\npython_version_number (3.10): 3.12 # (6)!\nSelect dataset_storage\n    1 - none\n    2 - azure\n    3 - s3\n    4 - gcs\n    Choose from [1/2/3/4] (1): # (7)!\nSelect environment_manager\n    1 - virtualenv\n    2 - conda\n    3 - pipenv\n    4 - uv\n    5 - pixi\n    6 - poetry\n    7 - none\n    Choose from [1/2/3/4/5/6/7] (1): 4 # (8)!\nSelect dependency_file\n    1 - requirements.txt\n    2 - pyproject.toml\n    3 - environment.yml\n    4 - Pipfile\n    5 - pixi.toml\n    Choose from [1/2/3/4/5] (1): 2 # (9)!\nSelect pydata_packages\n    1 - none\n    2 - basic\n    Choose from [1/2] (1): 2 # (10)!\nSelect testing_framework\n    1 - none\n    2 - pytest\n    3 - unittest\n    Choose from [1/2/3] (1): 2 # (11)!\nSelect linting_and_formatting\n    1 - ruff\n    2 - flake8+black+isort\n    Choose from [1/2] (1): 1 # (12)!\nSelect open_source_license\n    1 - No license file\n    2 - MIT\n    3 - BSD-3-Clause\n    Choose from [1/2/3] (1): 2 # (13)!\nSelect docs\n    1 - mkdocs\n    2 - none\n    Choose from [1/2] (1): 1 # (14)!\nSelect include_code_scaffold\n    1 - Yes\n    2 - No\n    Choose from [1/2] (1): 1 # (15)!\n</code></pre> <ol> <li>Project Name: We want a short and descriptive name for our project.</li> <li>Repository Name: This will be the name of the git repository, this defaults to the project name but can be changed.</li> <li>Module Name: This is the name of the main Python module for your code, again defaulting to the project name but can be changed.</li> <li>Author Name: Use your team or organization name for clarity.</li> <li>Description: A brief summary of the project's purpose.</li> <li>Python Version: Choose a modern, supported version (e.g., 3.12).</li> <li>Dataset Storage: Select <code>none</code> unless you plan to use cloud storage.</li> <li>Environment Manager: Choose <code>uv</code> if you've done the dependency management workshop. It won't create the virtual environment for you, but it will set up the configuration files.</li> <li>Dependency File: Choose <code>pyproject.toml</code> for modern Python projects.</li> <li>PyData Packages: Choose <code>basic</code> to include common data science libraries like pandas, numpy, and matplotlib.</li> <li>Testing Framework: Choose <code>pytest</code> for professional testing.</li> <li>Linting and Formatting: Choose <code>ruff</code> for fast, modern code quality checks.</li> <li>Open Source License: Choose <code>MIT</code> for open-source NHS work.</li> <li>Docs: Choose <code>mkdocs</code> if you plan to use what you have learned in the documentation workshop.</li> <li>Include Code Scaffold: Choose <code>Yes</code> to get example data processing scripts to help you get started.</li> </ol> <p>After answering all the prompts, CCDS will generate your new project in a directory named after your project. Try creating a project called <code>nhs-gp-appointment-analysis</code> to follow along with the example.</p>"},{"location":"workshops/cookiecutter_templates/#33-explore-your-new-project","title":"3.3 Explore Your New Project","text":"<p>Let's examine what CCDS created for us:</p> <pre><code># Navigate to your new project\ncd nhs-gp-appointment-analysis\n\n# See the project structure\ntree -L 2\n# Or if tree isn't available:\nfind . -type d -maxdepth 2 | sort\n</code></pre> <p>You can also just open it in an IDE of choice (e.g., VSCode, PyCharm) to explore the files and directories.</p> <p>Project Structure Generated</p> <p>CCDS created a complete project with:</p> <ul> <li>Organized directories for data, code, docs, and outputs</li> <li>Configuration files for dependencies and git</li> <li>Documentation templates to get you started</li> <li>Makefile for common tasks</li> <li>Git initialization ready for version control</li> </ul>"},{"location":"workshops/cookiecutter_templates/#task-4-understanding-the-ccds-structure","title":"Task 4: Understanding the CCDS Structure","text":"<p>Let's explore each part of your new project and understand its purpose.</p>"},{"location":"workshops/cookiecutter_templates/#41-data-organization","title":"4.1 Data Organization","text":"<p>The <code>data/</code> directory follows a clear data processing pipeline:</p> <pre><code># Explore the data structure\nls -la data/\n</code></pre> <p>Directory purposes: - <code>raw/</code> - Original, immutable data (never edit these files!) - <code>external/</code> - Third-party data sources - <code>interim/</code> - Partially processed data - <code>processed/</code> - Final, analysis-ready datasets</p> <p>Data Handling Best Practices</p> <p>DO: - \u2705 Keep raw data immutable - never edit original files - \u2705 Document data sources and processing steps - \u2705 Use version control for data processing scripts (not the data itself)</p> <p>DON'T: - \u274c Put large data files in git (use .gitignore) - \u274c Edit raw data files directly - \u274c Store personal or sensitive data without proper security</p>"},{"location":"workshops/cookiecutter_templates/#42-source-code-organization","title":"4.2 Source Code Organization","text":"<p>The <code>src/</code> directory organizes your code by function:</p> <pre><code># Explore the source code structure  \nls -la src/\n</code></pre> <p>Code organization: - <code>data/</code> - Scripts for downloading, cleaning, and processing data - <code>features/</code> - Code for feature engineering and data transformation - <code>models/</code> - Training scripts and model utilities - <code>visualization/</code> - Plotting and visualization functions</p>"},{"location":"workshops/cookiecutter_templates/#43-project-configuration","title":"4.3 Project Configuration","text":"<p>Let's examine the key configuration files:</p> <pre><code># Look at the project dependencies\ncat pyproject.toml\n\n# Check the README template\nhead -20 README.md\n\n# See what's ignored by git\ncat .gitignore\n</code></pre> <p>Configuration Benefits</p> <p>CCDS provides:</p> <ul> <li><code>pyproject.toml</code> - Modern Python dependency management</li> <li><code>requirements.txt</code> - Fallback for traditional pip workflows  </li> <li><code>.gitignore</code> - Sensible defaults for data science (excludes data files, models, etc.)</li> <li><code>Makefile</code> - Automated commands for common tasks</li> </ul>"},{"location":"workshops/cookiecutter_templates/#44-documentation-and-reports","title":"4.4 Documentation and Reports","text":"<pre><code># Explore documentation structure\nls -la docs/\nls -la reports/\n</code></pre> <p>Documentation structure: - <code>docs/</code> - Project documentation and guides - <code>reports/</code> - Generated analysis reports - <code>reports/figures/</code> - Charts and visualizations for reports</p>"},{"location":"workshops/cookiecutter_templates/#task-5-customizing-your-ccds-project","title":"Task 5: Customizing Your CCDS Project","text":"<p>Let's make this project truly yours by adding some initial content and configuration.</p>"},{"location":"workshops/cookiecutter_templates/#51-update-project-documentation","title":"5.1 Update Project Documentation","text":"<p>Edit the README.md to describe your specific project:</p> <pre><code># Open the README in your editor\n# Replace the template content with your project details\n</code></pre> <p>Include in your README: - Project overview - What problem are you solving? - Data sources - Where does your data come from? - Key findings - What have you discovered? (update as you progress) - How to reproduce - Instructions for running your analysis</p>"},{"location":"workshops/cookiecutter_templates/#52-set-up-dependencies","title":"5.2 Set Up Dependencies","text":"<p>Let's add some common data science dependencies to your project:</p> Modern approach (pyproject.toml)Traditional approach (requirements.txt) <p>Add these to your <code>pyproject.toml</code>:</p> <pre><code>[project]\ndependencies = [\n    \"pandas&gt;=2.0.0\",\n    \"numpy&gt;=1.24.0\", \n    \"matplotlib&gt;=3.7.0\",\n    \"seaborn&gt;=0.12.0\",\n    \"scikit-learn&gt;=1.3.0\",\n    \"jupyter&gt;=1.0.0\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest&gt;=7.0.0\",\n    \"ruff&gt;=0.1.0\",\n]\n</code></pre> <p>Add these to your <code>requirements.txt</code>:</p> <pre><code>pandas&gt;=2.0.0\nnumpy&gt;=1.24.0\nmatplotlib&gt;=3.7.0\nseaborn&gt;=0.12.0\nscikit-learn&gt;=1.3.0\njupyter&gt;=1.0.0\npytest&gt;=7.0.0\nruff&gt;=0.1.0\n</code></pre>"},{"location":"workshops/cookiecutter_templates/#53-initialize-version-control","title":"5.3 Initialize Version Control","text":"<p>Your project is ready for git:</p> <pre><code># Initialize git repository (if not already done)\ngit init\n\n# Add all files\ngit add .\n\n# Make your first commit\ngit commit -m \"Initial commit: CCDS project structure\n\nGenerated using Cookiecutter Data Science template for\nNHS GP appointment analysis project.\"\n</code></pre>"},{"location":"workshops/cookiecutter_templates/#54-create-your-first-notebook","title":"5.4 Create Your First Notebook","text":"<p>Let's create an initial analysis notebook:</p> <pre><code># Create a new notebook in the notebooks directory\ntouch notebooks/01-initial-data-exploration.ipynb\n</code></pre> <p>Notebook Naming Convention</p> <p>CCDS recommends numbering notebooks for clear progression:</p> <ul> <li><code>01-initial-data-exploration.ipynb</code></li> <li><code>02-data-cleaning-and-preprocessing.ipynb</code> </li> <li><code>03-exploratory-data-analysis.ipynb</code></li> <li><code>04-model-development.ipynb</code></li> <li><code>05-final-analysis-and-reporting.ipynb</code></li> </ul>"},{"location":"workshops/cookiecutter_templates/#task-6-working-with-your-ccds-project","title":"Task 6: Working with Your CCDS Project","text":"<p>Now let's see how to use the project structure for actual data science work.</p>"},{"location":"workshops/cookiecutter_templates/#61-using-the-makefile","title":"6.1 Using the Makefile","text":"<p>CCDS includes a Makefile with common commands:</p> <pre><code># See available make commands\nmake help\n</code></pre> <p>Common make commands: - <code>make data</code> - Download/generate data - <code>make clean</code> - Delete compiled files - <code>make lint</code> - Check code style - <code>make requirements</code> - Install dependencies</p>"},{"location":"workshops/cookiecutter_templates/#62-example-workflow","title":"6.2 Example Workflow","text":"<p>Here's how a typical CCDS workflow looks:</p> <ol> <li>Add raw data to <code>data/raw/</code></li> <li>Create processing scripts in <code>src/data/</code></li> <li>Generate clean data to <code>data/processed/</code></li> <li>Develop features using <code>src/features/</code></li> <li>Train models with <code>src/models/</code></li> <li>Create visualizations using <code>src/visualization/</code></li> <li>Generate reports in <code>reports/</code></li> </ol>"},{"location":"workshops/cookiecutter_templates/#63-integration-with-other-workshops","title":"6.3 Integration with Other Workshops","text":"<p>Your CCDS project works perfectly with other workshop tools:</p> <p>Dependency Management: <pre><code># If you've done the dependency management workshop\nuv sync  # Install dependencies from pyproject.toml\n</code></pre></p> <p>Documentation: <pre><code># If you've done the MkDocs workshop\nmkdocs new .  # Add documentation site to your project\n</code></pre></p> <p>Code Quality: <pre><code># If you've done the packaging workshop\nruff check src/  # Lint your source code\n</code></pre></p>"},{"location":"workshops/cookiecutter_templates/#best-practices-for-ccds-projects","title":"Best Practices for CCDS Projects","text":""},{"location":"workshops/cookiecutter_templates/#data-science-workflow","title":"Data Science Workflow","text":"<p>Follow the Data Science Process</p> <p>1. Understand the Problem - Document business requirements in <code>docs/</code> - Define success metrics clearly</p> <p>2. Explore the Data - Keep raw data untouched in <code>data/raw/</code> - Document data quality issues - Create initial notebooks for exploration</p> <p>3. Prepare the Data - Write reusable scripts in <code>src/data/</code> - Save processed data to <code>data/processed/</code> - Version your data processing pipeline</p> <p>4. Model and Analyze - Develop models in <code>src/models/</code> - Save trained models to <code>models/</code> - Create reproducible training scripts</p> <p>5. Communicate Results - Generate reports in <code>reports/</code> - Create visualizations for stakeholders - Document findings and recommendations</p>"},{"location":"workshops/cookiecutter_templates/#project-organization","title":"Project Organization","text":"<p>Keep It Organized</p> <p>DO: - \u2705 Use descriptive file names with dates/versions - \u2705 Document your analysis process in notebooks - \u2705 Write reusable functions in <code>src/</code> modules - \u2705 Keep notebooks clean and well-commented - \u2705 Regular git commits with clear messages</p> <p>DON'T: - \u274c Put everything in one massive notebook - \u274c Copy-paste code between notebooks - \u274c Mix exploration and production code - \u274c Forget to document your assumptions</p>"},{"location":"workshops/cookiecutter_templates/#team-collaboration","title":"Team Collaboration","text":"<p>Working with Teams</p> <p>Benefits for teams: - Onboarding - New team members know where everything is - Code review - Consistent structure makes reviews easier - Knowledge sharing - Clear documentation and organization - Reproducibility - Anyone can run your analysis</p> <p>Tips for collaboration: - Use clear commit messages - Document your analysis decisions - Share environment setup instructions - Regular code reviews and knowledge sharing</p>"},{"location":"workshops/cookiecutter_templates/#troubleshooting","title":"Troubleshooting","text":""},{"location":"workshops/cookiecutter_templates/#common-issues","title":"Common Issues","text":"<p>Template Generation Fails</p> <pre><code># Clear CCDS cache and try again\nrm -rf ~/.cookiecutters/\nuv tool run ccds\n</code></pre> <p>Dependencies Won't Install</p> <pre><code># Update pip and try again\npython -m pip install --upgrade pip\npip install -r requirements.txt\n</code></pre> <p>Git Issues</p> <pre><code># If git isn't initialized\ngit init\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre>"},{"location":"workshops/cookiecutter_templates/#checkpoint","title":"Checkpoint","text":"<p>Before finishing this workshop, verify you can:</p> <ul> <li> Install and use Cookiecutter to generate project templates</li> <li> Create a new data science project using the CCDS template</li> <li> Understand the purpose of each directory in the CCDS structure</li> <li> Customize the project with your own dependencies and documentation</li> <li> Follow best practices for data science project organization</li> <li> Integrate CCDS with other workshop tools (UV, git, etc.)</li> </ul>"},{"location":"workshops/cookiecutter_templates/#next-steps","title":"Next Steps","text":"<p>Excellent work! You now have a professional, standardized project structure that follows industry best practices.</p> <p>Continue building your skills:</p> <ul> <li>Dependency Management - Manage project dependencies professionally</li> <li>Packaging with pyproject.toml - Make your code installable and reusable</li> <li>Documentation with MkDocs - Create professional project documentation</li> <li>Pre-Commit Hooks - Automate code quality checks</li> <li>CI/CD with GitHub Actions - Automate testing and deployment</li> </ul> <p>Apply CCDS to real projects: - Use CCDS for your next data analysis project - Convert existing projects to follow CCDS structure - Create team guidelines based on CCDS principles</p> Additional Resources"},{"location":"workshops/cookiecutter_templates/#cookiecutter-data-science","title":"Cookiecutter Data Science","text":"<ul> <li>CCDS Official Website - Complete documentation and philosophy</li> <li>CCDS GitHub Repository - Source code and issues</li> <li>DrivenData Blog - Articles on data science best practices</li> <li>CCDS Philosophy - Why this structure works</li> </ul>"},{"location":"workshops/cookiecutter_templates/#cookiecutter-templates","title":"Cookiecutter Templates","text":"<ul> <li>Cookiecutter Documentation - Complete Cookiecutter guide</li> <li>Cookiecutter Templates - Other useful templates</li> <li>Creating Custom Templates - Build your own templates</li> </ul>"},{"location":"workshops/cookiecutter_templates/#data-science-project-management","title":"Data Science Project Management","text":"<ul> <li>Good Enough Practices - Academic paper on scientific computing best practices</li> <li>The Turing Way - Comprehensive guide to reproducible research</li> <li>Software Carpentry - Best practices for scientific software</li> </ul>"},{"location":"workshops/cookiecutter_templates/#nhs-and-healthcare-data-science","title":"NHS and Healthcare Data Science","text":"<ul> <li>RAP Community of Practice - NHS standards for reproducible analysis</li> <li>NHS Digital Data Science - NHS data standards and guidelines</li> <li>FAIR Data Principles - Making data Findable, Accessible, Interoperable, Reusable</li> </ul>"},{"location":"workshops/dependency_management/","title":"Dependency Management: From pip to the pyroject.toml to uv","text":"<p>Learn how to modernize your Python dependency management by building on traditional <code>pip + venv</code> workflows and transitioning to modern tools like <code>uv</code> for better organization and performance.</p> <p>Learning Objectives</p> <ul> <li>Master pip + venv fundamentals that form the foundation of Python dependency management</li> <li>Organize dependencies using <code>pyproject.toml</code> for better structure and maintainability</li> <li>Understand how simple requirements become complex dependency trees</li> <li>Introduce UV as a modern Python package manager built on solid foundations</li> <li>Create reproducible environments using lockfiles and dependency groups</li> </ul> Why This Matters for RAP <p>This workshop directly supports Silver RAP by teaching you to include comprehensive dependency information in your repository. You'll learn to structure dependencies using <code>pyproject.toml</code>, which not only ensures reproducibility but also shapes your analytical pipeline into a proper package - a key step toward Gold RAP's \"code is fully packaged\" requirement.</p>"},{"location":"workshops/dependency_management/#task-1-understanding-traditional-python-dependency-management","title":"Task 1: Understanding Traditional Python Dependency Management","text":"<p>Let's start by setting up a traditional Python environment to understand the current approach and its limitations.</p>"},{"location":"workshops/dependency_management/#11-create-a-virtual-environment","title":"1.1 Create a Virtual Environment","text":"<p>First, let's create a clean virtual environment using the standard <code>venv</code> module:</p> <p>Virtual Environment Basics</p> <p>Virtual environments isolate your project dependencies from your system Python installation. The <code>.venv</code> directory contains a complete Python installation specific to your project.</p> <pre><code># Create a new virtual environment\npython -m venv .venv\n\n# Activate the virtual environment\nsource .venv/bin/activate\n\n# Verify we're in the virtual environment\nwhich python\n</code></pre>"},{"location":"workshops/dependency_management/#12-examine-current-dependencies","title":"1.2 Examine Current Dependencies","text":"<p>Let's look at what dependencies our project needs:</p> <pre><code># View the current requirements file\ncat requirements.txt\n</code></pre> <p>You should see a mix of dependencies including documentation tools, development tools, and core project dependencies.</p>"},{"location":"workshops/dependency_management/#13-install-dependencies-and-observe-complexity","title":"1.3 Install Dependencies and Observe Complexity","text":"<p>Now let's install these dependencies and see what actually gets installed:</p> <pre><code># Install all requirements\npip install -r requirements.txt\n\n# See what was actually installed (this will be much longer!)\npip freeze\n</code></pre> <p>Dependency Explosion</p> <p>Notice how our simple requirements file with ~10 packages resulted in many more installed packages. These are sub-dependencies (dependencies of dependencies) that pip resolved automatically.</p>"},{"location":"workshops/dependency_management/#understanding-traditional-approach-limitations","title":"Understanding Traditional Approach Limitations","text":"<p>The traditional <code>pip + venv</code> approach works well for basic projects but has some challenges as projects grow:</p> <ul> <li>Mixed dependency purposes: Production, development, and documentation dependencies are all in one file</li> <li>Sub-dependency visibility: <code>pip freeze</code> shows all packages, making it hard to distinguish your direct dependencies</li> <li>Slower resolution: pip can be slow with complex dependency trees</li> <li>No built-in lockfiles: Reproducible environments require manual <code>pip freeze</code> management</li> </ul> <p><code>pip</code> and <code>venv</code> is still valid</p> <p>Don't worry - <code>pip</code> and <code>venv</code> is still a perfectly valid approach for many projects! We're building on this solid foundation, not replacing it entirely.</p>"},{"location":"workshops/dependency_management/#task-2-organizing-dependencies-with-pyprojecttoml","title":"Task 2: Organizing Dependencies with pyproject.toml","text":"<p>Before we introduce <code>uv</code>, let's improve our dependency organization using the modern <code>pyproject.toml</code> standard.</p>"},{"location":"workshops/dependency_management/#21-understanding-pyprojecttoml-structure","title":"2.1 Understanding pyproject.toml Structure","text":"<p>Complete pyproject.toml Guide</p> <p>This section focuses on dependency management within pyproject.toml. For comprehensive coverage of project metadata, dynamic versioning, and tool configuration, see our Packaging with pyproject.toml workshop.</p> <p>The <code>pyproject.toml</code> file is the modern standard for Python project configuration. For detailed guidance on writing pyproject.toml files, see the official writing guide. Let's examine our current minimal setup:</p> <pre><code># View current pyproject.toml\ncat pyproject.toml\n</code></pre>"},{"location":"workshops/dependency_management/#22-add-project-dependencies","title":"2.2 Add Project Dependencies","text":"<p>Let's organize our dependencies by purpose. Open <code>pyproject.toml</code> and add the following sections:</p> <pre><code>[project] # (1)!\nname = \"package-your-code-workshop\"\nversion = \"0.1.0\"\ndescription = \"A workshop demonstrating Python packaging best practices\"\ndependencies = [ # (2)!\n    \"pandas&gt;=2.1.0\",\n    \"numpy&gt;=1.25.0\",\n    \"matplotlib&gt;=3.7.0\",\n    \"seaborn&gt;=0.12.0\",\n    \"plotly&gt;=5.15.0\",\n    \"oops_its_a_pipeline@git+https://github.com/nhsengland/oops-its-a-pipeline.git\", # (3)!\n    \"nhs_herbot@git+https://github.com/nhsengland/nhs_herbot.git\",\n]\n\n[dependency-groups] # (4)!\ndocs = [ # (5)!\n    \"mkdocs&gt;=1.5.0\",\n    \"mkdocs-material&gt;=9.0.0\",\n    \"mkdocstrings&gt;=0.22.0\",\n    \"mkdocstrings-python&gt;=1.0.0\",\n]\ndev = [ # (6)!\n    \"ruff&gt;=0.4.0\",\n    \"pytest&gt;=7.4.0\",\n]\n\n[tool.setuptools.packages.find] # (7)!\ninclude = [\"practice_level_gp_appointments*\"]\n</code></pre> <ol> <li>Core project metadata section following PEP 621</li> <li>Core dependencies required for your application to run in production</li> <li>Git-based dependencies - packages installed directly from repositories</li> <li>Dependency groups for development tools following PEP 735</li> <li>Documentation generation dependencies - only needed when building docs</li> <li>Development tools - only needed when coding and testing</li> <li>Build tool configuration - tells setuptools which packages to include</li> </ol> <p>Why Separate Groups?</p> <p>Now you can install exactly what you need:</p> <pre><code># Traditional approach - everything mixed together\npip install -r requirements.txt  # ~50 packages\n\n# Modern approach - install selectively\npip install -e .         # Core dependencies only\npip install -e .[dev]    # Core + development tools\npip install -e .[docs]   # Core + documentation tools\n</code></pre> Dependency Groups: Modern Best Practice <p>We're using <code>dependency-groups</code> as the modern best practice for development tools:</p> <pre><code># Modern approach - dependency groups for dev tools\n[dependency-groups]\ndev = [\"pytest\", \"ruff\"]\ndocs = [\"mkdocs\", \"mkdocs-material\"]\n</code></pre> <p>Dependency groups are specifically designed for development tools, testing, and build processes. They're supported by modern tools like UV and newer versions of pip.</p> <p>For backwards compatibility with older pip versions, you can still use: <pre><code># Fallback approach - optional dependencies\n[project.optional-dependencies]\ndev = [\"pytest\", \"ruff\"]\ndocs = [\"mkdocs\", \"mkdocs-material\"]  \n</code></pre></p>"},{"location":"workshops/dependency_management/#23-test-the-new-structure","title":"2.3 Test the New Structure","text":"<p>Let's clean our environment and test our new dependency structure:</p> <pre><code># Deactivate and remove the old environment\ndeactivate\nrm -rf .venv\n\n# Create a fresh environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install just core dependencies\npip install -e .\n\n# Test that our package is accessible\npython -c \"import practice_level_gp_appointments; print('Success')\"\n\n# Now install development tools too\npip install -e .[dev]\n\n# Install everything\npip install -e .[dev,docs]\n</code></pre>"},{"location":"workshops/dependency_management/#task-3-introducing-uv","title":"Task 3: Introducing UV","text":"<p>Now let's introduce UV, a modern Python package manager built in Rust that builds on the foundations we've established.</p>"},{"location":"workshops/dependency_management/#31-install-uv","title":"3.1 Install UV","text":"<p>Let's install UV on your system:</p> <pre><code># Install UV (macOS/Linux)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Restart your shell or source the new PATH\nsource ~/.bashrc  # or ~/.zshrc depending on your shell # (1)!\n\n# Verify installation\nuv --version\n</code></pre> <ol> <li>To check the type of shell you're using, run <code>echo $SHELL</code>. If it ends with <code>zsh</code>, use <code>source ~/.zshrc</code> instead.</li> </ol> <p>Windows Installation</p> <p>On Windows, use: <code>powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"</code>. For more installation options, see the UV installation guide.</p>"},{"location":"workshops/dependency_management/#32-migrate-existing-project-to-uv","title":"3.2 Migrate Existing Project to UV","text":"<p>Let's migrate our existing project to use UV while keeping our pyproject.toml structure. For a comprehensive migration guide, see Migrating from pip to a UV project:</p> <pre><code># First, clean the current environment\ndeactivate\nrm -rf .venv\n\n# Create a UV-managed virtual environment\nuv venv\n\n# Activate the environment\nsource .venv/bin/activate\n\n# Install dependencies from pyproject.toml\nuv sync --all-groups\n</code></pre> <p>UV Sync Command</p> <p><code>uv sync</code> reads your <code>pyproject.toml</code> and installs all dependencies. The <code>--all-groups</code> flag includes all dependency groups (dev, docs, etc.).</p>"},{"location":"workshops/dependency_management/#33-practice-selective-installation-with-uv","title":"3.3 Practice: Selective Installation with UV","text":"<p>Now let's practice using UV's dependency groups with the same <code>optional-dependencies</code> syntax.</p>"},{"location":"workshops/dependency_management/#practice-different-installation-patterns","title":"Practice Different Installation Patterns","text":"<p>Let's practice installing dependencies for different use cases using our existing pyproject.toml structure:</p> <pre><code># Start with a clean slate\ndeactivate\nrm -rf .venv\n\n# 1. Core dependencies only (production-like)\nuv venv\nsource .venv/bin/activate\nuv sync\npip list  # See what got installed\n\n# 2. Add development tools\nuv sync --group dev\npip list  # Notice the additional packages\n\n# 3. Clean and try docs only\ndeactivate\nrm -rf .venv\nuv venv\nsource .venv/bin/activate\nuv sync --group docs\npip list  # Just core + docs packages\n\n# 4. Everything for full development\nuv sync --all-groups\npip list  # All packages\n</code></pre> Real-World Scenarios <p>New Developer Setup: A developer working on code (not docs) <pre><code>uv sync --group dev\n</code></pre></p> <p>Documentation Writer: Someone updating docs (not coding) <pre><code>uv sync --group docs\n</code></pre></p> <p>Production Deployment: Server needs only core functionality <pre><code>uv sync  # No groups = core only\n</code></pre></p> <p>CI/CD Pipeline: Different jobs, different needs <pre><code># Testing job\n- run: uv sync --group dev\n\n# Documentation job  \n- run: uv sync --group docs\n\n# Production deployment\n- run: uv sync\n</code></pre></p>"},{"location":"workshops/dependency_management/#34-alternative-building-a-project-from-scratch-with-uv","title":"3.4 Alternative: Building a Project from Scratch with UV","text":"<p>Let's also practice the \"greenfield\" approach - starting a completely new project with UV:</p> <pre><code># Clean everything\ndeactivate\nrm -rf .venv uv.lock\n\n# Initialize a new UV project\nuv init --name package-your-code-workshop --python 3.12\n\n# Create and activate environment\nuv venv --python 3.12\nsource .venv/bin/activate\n\n# Add dependencies one by one (UV builds pyproject.toml automatically)\nuv add pandas numpy matplotlib seaborn plotly\n\n# Add development dependencies\nuv add --group dev ruff pytest\n\n# Add documentation dependencies  \nuv add --group docs mkdocs mkdocs-material mkdocstrings mkdocstrings-python\n\n# Check what UV created\ncat pyproject.toml\n</code></pre> UV Auto-Generation <p>UV automatically creates and updates your <code>pyproject.toml</code> as you add dependencies. This is great for new projects where you want to build up dependencies incrementally.</p>"},{"location":"workshops/dependency_management/#35-understanding-uv-lockfiles","title":"3.5 Understanding UV Lockfiles","text":"<p>UV automatically creates a <code>uv.lock</code> file for reproducible builds. Let's explore it:</p> <pre><code># Check if lockfile exists\nls -la uv.lock\n\n# Look at the lockfile structure\nhead -20 uv.lock\n\n# Install from exact lockfile versions\nuv sync --frozen\n</code></pre> <p>Always Commit Lockfiles</p> <p>Add <code>uv.lock</code> to version control to ensure everyone gets exactly the same dependency versions. This is essential for RAP Gold standard reproducibility - your analytical pipelines will run identically across different environments and team members.</p>"},{"location":"workshops/dependency_management/#task-4-working-with-uv-in-practice","title":"Task 4: Working with UV in Practice","text":"<p>Let's explore common UV workflows you'll use in daily development. For comprehensive guidance on UV project workflows, see the Working on Projects guide.</p>"},{"location":"workshops/dependency_management/#41-adding-and-removing-dependencies","title":"4.1 Adding and Removing Dependencies","text":"<pre><code># Add a new dependency\nuv add requests\n\n# Add a development dependency\nuv add --group dev mypy\n\n# Remove a dependency\nuv remove requests\n\n# Upgrade all dependencies\nuv lock --upgrade\n</code></pre>"},{"location":"workshops/dependency_management/#42-managing-environments","title":"4.2 Managing Environments","text":"<pre><code># Create environment with specific Python version\nuv venv --python 3.11\n\n# List available Python versions\nuv python list\n\n# Install a specific Python version (if needed)\nuv python install 3.11\n</code></pre>"},{"location":"workshops/dependency_management/#43-running-commands","title":"4.3 Running Commands","text":"<pre><code># Run commands in the UV environment\nuv run python --version\n\n# Run the package as a module (uses __main__.py)\nuv run python -m practice_level_gp_appointments\n\n# Run a specific script file\nuv run python practice_level_gp_appointments/pipeline.py\n\n# Run tools from your environment\nuv run ruff check .\n</code></pre> <p>UV Run</p> <p><code>uv run</code> automatically activates the virtual environment and runs the command, even if you haven't manually activated the environment.</p>"},{"location":"workshops/dependency_management/#migration-command-reference","title":"Migration Command Reference","text":"<p>Here's a quick reference for migrating from pip workflows to UV:</p> Traditional pip Modern UV Purpose <code>pip install package</code> <code>uv add package</code> Add new dependency <code>pip install -r requirements.txt</code> <code>uv sync</code> Install all dependencies <code>pip install -e .</code> <code>uv sync</code> Install project in development mode <code>pip freeze &gt; requirements.txt</code> <code>uv export &gt; requirements.txt</code> Export current environment <code>pip install --upgrade package</code> <code>uv add package --upgrade</code> Upgrade package <code>python script.py</code> <code>uv run python script.py</code> Run Python script Command Details <p>Key differences to note:</p> <ul> <li><code>uv sync</code> installs your project and dependencies from <code>pyproject.toml</code></li> <li><code>uv export</code> creates requirements.txt from the current environment</li> <li><code>uv lock</code> updates the lockfile (separate from installation)</li> <li>All UV commands automatically handle virtual environments</li> </ul>"},{"location":"workshops/dependency_management/#best-practices","title":"Best Practices","text":""},{"location":"workshops/dependency_management/#dependency-group-organization","title":"Dependency Group Organization","text":"<p>Keep It Simple: Two Groups</p> <p>For most projects, you only need two dependency groups:</p> <ul> <li>Core dependencies (<code>dependencies</code>): What your app needs to run</li> <li>Development dependencies (<code>dev</code>): Tools for coding, testing, linting</li> </ul> <p>Simple, effective setup: <pre><code>[project]\ndependencies = [\"pandas\", \"requests\"]\n\n[dependency-groups]\ndev = [\"pytest\", \"ruff\"]\n</code></pre></p> <p>Installation: <pre><code># With UV (modern)\nuv sync --group dev\n\n# With pip (fallback - use optional-dependencies)\npip install -e .[dev]\n</code></pre></p> Advanced: More Granular Groups <p>If your project grows complex, you can break down further:</p> <ul> <li><code>docs</code>: Documentation generation tools</li> <li><code>test</code>: Testing-specific dependencies (separate from general dev)</li> <li><code>typing</code>: Type checking tools (mypy, type stubs)</li> <li><code>jupyter</code>: Jupyter notebook dependencies</li> </ul> <p>Example comprehensive setup: <pre><code>[dependency-groups]\ndev = [\"ruff\", \"pytest\"]\ntest = [\"pytest\", \"pytest-cov\"]\ndocs = [\"mkdocs\", \"mkdocs-material\"]\ntyping = [\"mypy\", \"types-requests\"]\n</code></pre></p> <p>But honestly, most projects don't need this complexity!</p>"},{"location":"workshops/dependency_management/#installation-patterns","title":"Installation Patterns","text":"<p>Two Commands You'll Use Most</p> <pre><code># Production deployment\nuv sync\n\n# Development work  \nuv sync --group dev\n</code></pre> <p>That's it! Simple and effective.</p> Other Installation Options <pre><code># Install everything (if you have multiple groups)\nuv sync --all-groups\n\n# Install specific groups only\nuv sync --group docs\nuv sync --group test\n\n# Multiple specific groups\nuv sync --group dev --group test\n</code></pre>"},{"location":"workshops/dependency_management/#working-on-locked-down-platforms","title":"Working on Locked-Down Platforms","text":"<p>When You Can't Install UV</p> <p>Many enterprise/NHS environments don't allow installing new tools like UV. The good news? The organized dependency structure still helps with traditional pip!</p> <p>With organized pyproject.toml, you can still benefit: <pre><code># Use pip with optional dependencies\npip install -e .              # Core dependencies only\npip install -e .[dev]         # Core + development tools\n\n# Or export to requirements files for teams\nuv export --group dev &gt; requirements-dev.txt  # (when UV is available)\n# Then share requirements-dev.txt for pip users\npip install -r requirements-dev.txt\n</code></pre></p> <p>Key benefits even with just pip: - Clear separation of production vs development dependencies - Easy to share specific requirement sets with team members - Future-ready when you can eventually use modern tools like Poetry or Hatch - Better project organization and documentation</p>"},{"location":"workshops/dependency_management/#do-this","title":"Do This","text":"<ul> <li>Organize dependencies: Use <code>pyproject.toml</code> to separate production and development dependencies</li> <li>Use the tools available: UV when possible, pip when necessary - both work with organized dependencies</li> <li>Pin appropriately: Use <code>&gt;=</code> for minimum versions, avoid overly specific pins</li> <li>Document your setup: Make it clear how team members should install dependencies</li> <li>Plan for constraints: Consider locked-down environments when choosing your approach</li> </ul>"},{"location":"workshops/dependency_management/#avoid-this","title":"Avoid This","text":"<ul> <li>All dependencies in one place: Don't mix production and development dependencies</li> <li>Unpinned dependencies: Specify minimum versions for stability</li> <li>Over-pinning: Avoid exact version pins unless absolutely necessary</li> <li>Assuming everyone can use modern tools: Not everyone can install UV on their systems</li> </ul>"},{"location":"workshops/dependency_management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"workshops/dependency_management/#common-issues","title":"Common Issues","text":"<p>Package Not Found</p> <pre><code># Clear cache and retry\nuv cache clean\nuv sync --all-groups\n</code></pre> <p>Version Conflicts</p> <pre><code># Use UV's conflict resolution options\nuv add package-name --resolution lowest-direct\n</code></pre> <p>Environment Issues</p> <pre><code># Start completely fresh\nrm -rf .venv uv.lock\nuv venv\nuv sync --all-groups\n</code></pre>"},{"location":"workshops/dependency_management/#checkpoint","title":"Checkpoint","text":"<p>Before moving to the next workshop, verify you can:</p> <ul> <li> Create and activate virtual environments with both <code>venv</code> and <code>uv</code></li> <li> Understand the difference between direct and sub-dependencies</li> <li> Organize dependencies in <code>pyproject.toml</code> using dependency groups</li> <li> Install dependencies with both <code>pip</code> and <code>uv sync</code></li> <li> Add and remove packages using UV commands</li> <li> Understand the purpose of <code>uv.lock</code> files</li> </ul>"},{"location":"workshops/dependency_management/#next-steps","title":"Next Steps","text":"<p>Excellent work! You've successfully modernized your dependency management workflow while building on solid <code>pip + venv</code> foundations.</p> <p>Continue your learning journey - these workshops can be done in any order:</p> <ul> <li>Packaging with pyproject.toml - Make your code installable and reusable</li> <li>Documentation with MkDocs - Create professional documentation</li> <li>Pre-Commit Hooks - Automate code quality checks</li> <li>CI/CD with GitHub Actions - Automate testing and deployment</li> </ul> Additional Resources"},{"location":"workshops/dependency_management/#rap-community-of-practice","title":"RAP Community of Practice","text":"<ul> <li>Why Use Virtual Environments - RAP guidance on virtual environments for reproducible analysis</li> </ul>"},{"location":"workshops/dependency_management/#uv-modern-python-package-manager","title":"UV (Modern Python Package Manager)","text":"<ul> <li>UV Documentation - Complete UV guide and reference</li> <li>UV Working on Projects - Practical UV project workflows</li> <li>UV Migration Guide - Step-by-step pip to UV migration</li> <li>UV vs pip Comparison - Detailed comparison of tools</li> </ul>"},{"location":"workshops/dependency_management/#python-project-configuration","title":"Python Project Configuration","text":"<ul> <li>Writing pyproject.toml - Official guide to pyproject.toml</li> <li>PEP 621 - Project Metadata - Standard for pyproject.toml</li> <li>PEP 735 - Dependency Groups - Modern dependency organization</li> </ul>"},{"location":"workshops/dependency_management/#traditional-python-packaging","title":"Traditional Python Packaging","text":"<ul> <li>Python Packaging Guide - Official Python packaging documentation</li> <li>Virtual Environments Guide - Python.org official guide</li> <li>pip User Guide - Official pip documentation</li> </ul>"},{"location":"workshops/dependency_management/#best-practices-standards","title":"Best Practices &amp; Standards","text":"<ul> <li>Python Packaging Best Practices - Official packaging guidelines</li> <li>Understanding Semantic Versioning - Version specification standards</li> <li>Python Enhancement Proposals (PEPs) - Python standards and proposals</li> </ul>"},{"location":"workshops/github_actions/","title":"CI/CD with GitHub Actions: Automate Testing and Documentation Deployment","text":"<p>Bonus Workshop - Self-Paced</p> <p>This is an optional, self-paced workshop. You can complete it at your own speed and refer back to it as needed.</p> <p>CI/CD Definitions</p> <p>Continuous Integration (CI) automatically tests your code every time you make changes, catching problems early.</p> <p>Continuous Deployment (CD) automatically deploys your applications when changes are merged, ensuring your users always have the latest version.</p> <p>GitHub Actions is GitHub's built-in CI/CD platform that runs workflows when events happen in your repository (like pushes, pull requests, or releases).</p>"},{"location":"workshops/github_actions/#why-use-github-actions","title":"Why Use GitHub Actions?","text":"<p>Catch Issues Early: Test every change before it's merged into main Automate Repetitive Tasks: No more manual testing and deployment Consistent Quality: Same checks run for everyone, every time Fast Feedback: Know immediately if your changes break something Always Up-to-Date Docs: Documentation deploys automaticallytip \"Bonus Workshop - Self-Paced\"     This is an optional, self-paced workshop. You can complete it at your own speed and refer back to it as needed.</p> <p>Learn how to automate code quality checks, testing, and documentation deployment using GitHub Actions - making your development workflow more professional and reliable.</p> <p>Learning Objectives</p> <ul> <li>Understand GitHub Actions fundamentals and workflow structure</li> <li>Create automated code quality checks with Ruff and pytest</li> <li>Set up conditional documentation building and testing</li> <li>Automate documentation deployment to GitHub Pages</li> <li>Follow CI/CD best practices for Python projects</li> </ul> Why This Matters for RAP <p>Automated testing and deployment directly supports Silver RAP by ensuring code quality and Gold RAP by automating the publication process. GitHub Actions helps you maintain consistent code standards, catch issues early, and deploy documentation automatically - essential for professional analytical pipelines.</p> What is CI/CD? <ul> <li>Continuous Integration (CI) automatically tests your code every time you make changes, catching problems early.</li> <li>Continuous Deployment (CD) automatically deploys your applications when changes are merged, ensuring your users always have the latest version.</li> <li>GitHub Actions is GitHub's built-in CI/CD platform that runs workflows when events happen in your repository (like pushes, pull requests, or releases).</li> </ul> <p>Why Use GitHub Actions?</p> <ul> <li>Catch Issues Early: Test every change before it's merged into main</li> <li>Automate Repetitive Tasks: No more manual testing and deployment</li> <li>Consistent Quality: Same checks run for everyone, every time</li> <li>Fast Feedback: Know immediately if your changes break something</li> <li>Always Up-to-Date Docs: Documentation deploys automatically</li> </ul>"},{"location":"workshops/github_actions/#task-1-understanding-github-actions-structure","title":"Task 1: Understanding GitHub Actions Structure","text":"<p>Let's start by understanding how GitHub Actions workflows are organized and what we'll build.</p>"},{"location":"workshops/github_actions/#11-workflow-structure","title":"1.1 Workflow Structure","text":"<p>GitHub Actions workflows are defined in YAML files stored in <code>.github/workflows/</code> directory. Each workflow consists of:</p> <ul> <li>Triggers (when to run): push, pull request, schedule, etc.</li> <li>Jobs (what to do): groups of steps that run together</li> <li>Steps (individual tasks): run commands, use actions, etc.</li> </ul>"},{"location":"workshops/github_actions/#12-our-cicd-strategy","title":"1.2 Our CI/CD Strategy","text":"<p>We'll create two workflows following the KISS principle:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Pull Request      \u2502    \u2502    Main Branch       \u2502\n\u2502   Quality Checks    \u2502    \u2502    Documentation     \u2502\n\u2502                     \u2502    \u2502    Deployment        \u2502\n\u2502 \u2713 Code Quality      \u2502    \u2502                      \u2502\n\u2502 \u2713 Tests             \u2502    \u2502 \u2713 Build Docs         \u2502\n\u2502 \u2713 Docs Build        \u2502    \u2502 \u2713 Deploy to Pages    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Workflow 1: Quality Checks (runs on pull requests) - Code quality with Ruff - Run pytest test suite - Test documentation builds (if docs changed)</p> <p>Workflow 2: Deploy Documentation (runs on main branch) - Build and deploy documentation to GitHub Pages</p>"},{"location":"workshops/github_actions/#task-2-setting-up-quality-checks-workflow","title":"Task 2: Setting Up Quality Checks Workflow","text":"<p>Let's create our first workflow to automatically check code quality when pull requests are opened.</p>"},{"location":"workshops/github_actions/#21-create-the-workflows-directory","title":"2.1 Create the Workflows Directory","text":"<p>First, create the directory structure for GitHub Actions:</p> <pre><code># Create the workflows directory\nmkdir -p .github/workflows\n\n# Verify the structure\nls -la .github/\n</code></pre>"},{"location":"workshops/github_actions/#22-create-the-quality-checks-workflow","title":"2.2 Create the Quality Checks Workflow","text":"<p>Create a new file <code>.github/workflows/quality-checks.yml</code> and choose the version that matches your project setup:</p> With pyproject.tomlWith requirements.txt <p>Use this version if you've completed the dependency management workshop:</p> <pre><code>name: Quality Checks # (1)!\n\non: # (2)!\n  pull_request:\n    branches: [ main ]\n  push:\n    branches: [ main ]\n\njobs:\n  quality-checks: # (3)!\n    runs-on: ubuntu-latest # (4)!\n\n    steps:\n    - name: Checkout code # (5)!\n      uses: actions/checkout@v4\n\n    - name: Set up Python # (6)!\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.12'\n\n    - name: Install dependencies # (7)!\n      run: |\n        pip install ruff pytest\n        pip install -e .\n\n    - name: Run Ruff linting # (8)!\n      run: ruff check practice_level_gp_appointments/\n\n    - name: Run Ruff formatting check # (9)!\n      run: ruff format --check practice_level_gp_appointments/\n\n    - name: Run pytest # (10)!\n      run: pytest tests/ -v\n\n  docs-check: # (11)!\n    runs-on: ubuntu-latest\n    if: contains(github.event.pull_request.changed_files, 'docs/') || github.event_name == 'push' # (12)!\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.12'\n\n    - name: Install dependencies # (13)!\n      run: |\n        pip install mkdocs mkdocs-material\n        pip install -e .\n\n    - name: Test documentation build # (14)!\n      run: mkdocs build --strict\n</code></pre> <p>Use this version if you haven't set up pyproject.toml yet:</p> <pre><code>name: Quality Checks # (1)!\n\non: # (2)!\n  pull_request:\n    branches: [ main ]\n  push:\n    branches: [ main ]\n\njobs:\n  quality-checks: # (3)!\n    runs-on: ubuntu-latest # (4)!\n\n    steps:\n    - name: Checkout code # (5)!\n      uses: actions/checkout@v4\n\n    - name: Set up Python # (6)!\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.12'\n\n    - name: Install dependencies # (7)!\n      run: |\n        pip install ruff pytest\n        pip install -r requirements.txt\n\n    - name: Run Ruff linting # (8)!\n      run: ruff check practice_level_gp_appointments/\n\n    - name: Run Ruff formatting check # (9)!\n      run: ruff format --check practice_level_gp_appointments/\n\n    - name: Run pytest # (10)!\n      run: pytest tests/ -v\n\n  docs-check: # (11)!\n    runs-on: ubuntu-latest\n    if: contains(github.event.pull_request.changed_files, 'docs/') || github.event_name == 'push' # (12)!\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.12'\n\n    - name: Install dependencies # (13)!\n      run: |\n        pip install mkdocs mkdocs-material\n        pip install -r requirements.txt\n\n    - name: Test documentation build # (14)!\n      run: mkdocs build --strict\n</code></pre> <ol> <li>Workflow name - appears in GitHub Actions UI</li> <li>Triggers - when this workflow runs (pull requests and pushes to main)</li> <li>First job - runs code quality checks</li> <li>Runner environment - uses latest Ubuntu (free for public repos)</li> <li>Checkout step - downloads your repository code</li> <li>Python setup - installs Python 3.12</li> <li>Dependencies - installs tools and your project dependencies</li> <li>Linting - checks code quality with Ruff</li> <li>Formatting - ensures consistent code formatting</li> <li>Testing - runs the test suite with pytest</li> <li>Second job - checks documentation building</li> <li>Conditional execution - only runs if docs files changed</li> <li>Documentation dependencies - installs MkDocs and your package</li> <li>Documentation test - ensures docs build without errors</li> </ol>"},{"location":"workshops/github_actions/#23-understanding-the-workflow","title":"2.3 Understanding the Workflow","text":"<p>Let's break down what this workflow does:</p> <p>Two Separate Jobs</p> <p>We split this into two jobs that run in parallel:</p> <ul> <li><code>quality-checks</code> - Always runs, checks code and tests</li> <li><code>docs-check</code> - Only runs if documentation files changed</li> </ul> <p>This saves time and resources when you're only changing code (not docs).</p> <p>Why <code>--strict</code> for MkDocs?</p> <p>The <code>--strict</code> flag makes MkDocs fail if there are any warnings. This catches issues like:</p> <ul> <li>Broken internal links</li> <li>Missing pages in navigation  </li> <li>Invalid markdown syntax</li> <li>Plugin configuration errors</li> </ul>"},{"location":"workshops/github_actions/#task-3-creating-the-documentation-deployment-workflow","title":"Task 3: Creating the Documentation Deployment Workflow","text":"<p>Now let's create a workflow that automatically deploys documentation to GitHub Pages when changes are merged to main.</p>"},{"location":"workshops/github_actions/#31-create-the-deployment-workflow","title":"3.1 Create the Deployment Workflow","text":"<p>Create <code>.github/workflows/deploy-docs.yml</code>:</p> With pyproject.tomlWith requirements.txt <pre><code>name: Deploy Documentation # (1)!\n\non: # (2)!\n  push:\n    branches: [ main ]\n    paths: [ 'docs/**', 'mkdocs.yml', 'pyproject.toml' ]\n\npermissions: # (3)!\n  contents: write\n\njobs:\n  deploy-docs:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code # (4)!\n      uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n\n    - name: Set up Python # (5)!\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.12'\n\n    - name: Install dependencies # (6)!\n      run: |\n        pip install mkdocs mkdocs-material\n        pip install -e .\n\n    - name: Deploy to GitHub Pages # (7)!\n      run: mkdocs gh-deploy --force\n</code></pre> <pre><code>name: Deploy Documentation # (1)!\n\non: # (2)!\n  push:\n    branches: [ main ]\n    paths: [ 'docs/**', 'mkdocs.yml', 'requirements.txt' ]\n\npermissions: # (3)!\n  contents: write\n\njobs:\n  deploy-docs:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout code # (4)!\n      uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n\n    - name: Set up Python # (5)!\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.12'\n\n    - name: Install dependencies # (6)!\n      run: |\n        pip install mkdocs mkdocs-material\n        pip install -r requirements.txt\n\n    - name: Deploy to GitHub Pages # (7)!\n      run: mkdocs gh-deploy --force\n</code></pre> <ol> <li>Deployment workflow - focuses only on publishing documentation</li> <li>Specific triggers - only runs when docs-related files change on main</li> <li>Permissions - only needs write access to push to gh-pages branch</li> <li>Full git history - fetch-depth: 0 gets complete history for gh-deploy</li> <li>Python setup - installs Python 3.12</li> <li>Install dependencies - gets MkDocs and your project dependencies</li> <li>Deploy with MkDocs - builds and deploys to GitHub Pages in one command</li> </ol>"},{"location":"workshops/github_actions/#32-configure-github-pages","title":"3.2 Configure GitHub Pages","text":"<p>Skip This Step If...</p> <p>If you've already completed the MkDocs Documentation workshop, you can skip this step as GitHub Pages is already configured for your repository.</p> <p>If you haven't set up GitHub Pages yet, you need to configure it to serve from the gh-pages branch:</p> <ol> <li>Go to your repository on GitHub</li> <li>Click Settings tab</li> <li>Scroll down to Pages section</li> <li>Under \"Source\" select \"Deploy from a branch\"</li> <li>Choose \"gh-pages\" branch and \"/ (root)\" folder</li> <li>Click Save</li> </ol> <p>No gh-pages Branch Yet?</p> <p>Don't worry! The <code>mkdocs gh-deploy</code> command will create the gh-pages branch automatically on first deployment. GitHub Pages will show an error until we deploy for the first time.</p>"},{"location":"workshops/github_actions/#task-4-testing-your-workflows","title":"Task 4: Testing Your Workflows","text":"<p>Now let's test both workflows to make sure they work correctly.</p>"},{"location":"workshops/github_actions/#41-test-the-quality-checks-workflow","title":"4.1 Test the Quality Checks Workflow","text":"<p>First, let's test the quality checks by creating a pull request:</p> <pre><code># Create a new branch\ngit checkout -b test-github-actions\n\n# Add the workflow files\ngit add .github/workflows/\n\n# Commit the changes\ngit commit -m \"feat: add GitHub Actions workflows for CI/CD\n\n- Add quality checks workflow for pull requests\n- Add documentation deployment workflow\n- Includes code linting, testing, and docs building\"\n\n# Push the branch\ngit push origin test-github-actions\n</code></pre> <p>Now create a pull request on GitHub:</p> <ol> <li>Go to your repository on GitHub</li> <li>Click \"Compare &amp; pull request\" </li> <li>Add a title: \"Add GitHub Actions CI/CD workflows\"</li> <li>Add a description explaining what you've added</li> <li>Click \"Create pull request\"</li> </ol> <p>Watch It Work</p> <p>After creating the pull request, you should see the GitHub Actions workflows start running automatically. Click on the \"Checks\" tab to see the progress.</p>"},{"location":"workshops/github_actions/#42-trigger-a-documentation-deployment","title":"4.2 Trigger a Documentation Deployment","text":"<p>Once your pull request is merged, test the documentation deployment:</p> <pre><code># Switch back to main and pull the changes\ngit checkout main\ngit pull origin main\n\n# Make a small change to documentation\necho \"This page was last updated: $(date)\" &gt;&gt; docs/content/index.md\n\n# Commit and push\ngit add docs/content/index.md\ngit commit -m \"docs: add last updated timestamp\"\ngit push origin main\n</code></pre> <p>This should trigger the documentation deployment workflow automatically.</p>"},{"location":"workshops/github_actions/#43-verify-everything-works","title":"4.3 Verify Everything Works","text":"<p>Check that both workflows are working:</p> <ol> <li>Go to Actions tab in your GitHub repository</li> <li>Verify quality checks ran on your pull request</li> <li>Verify docs deployment ran when you pushed to main</li> <li>Check your GitHub Pages site is updated</li> </ol> <p>Your documentation should be available at: <code>https://YOUR-USERNAME.github.io/package-your-code-workshop</code></p>"},{"location":"workshops/github_actions/#task-5-understanding-and-debugging-workflow-results","title":"Task 5: Understanding and Debugging Workflow Results","text":"<p>Let's learn how to interpret workflow results and debug issues when they occur.</p>"},{"location":"workshops/github_actions/#51-reading-workflow-status","title":"5.1 Reading Workflow Status","text":"<p>After your workflows run, GitHub Actions provides clear visual indicators in the repository:</p> <ol> <li>Navigate to the Actions tab in your GitHub repository</li> <li>Look at the workflow status icons next to each workflow run</li> <li>Click on any workflow to see detailed results</li> </ol> <p>Green checkmark - All checks passed</p> <p>Your workflow completed successfully. All tests passed, code quality checks passed, and any deployment steps succeeded.</p> <p>Red X - Something failed</p> <p>One or more steps in your workflow failed. Click on the workflow to see which step failed and why.</p> <p>Yellow dot - Workflow is running</p> <p>Your workflow is currently executing. You can click to watch the progress in real-time.</p> <p>Gray circle - Workflow was skipped</p> <p>The workflow didn't run, usually because the trigger conditions weren't met (e.g., no documentation files changed).</p>"},{"location":"workshops/github_actions/#52-debug-a-failed-workflow","title":"5.2 Debug a Failed Workflow","text":"<p>When a workflow fails, follow these steps to identify and fix the problem:</p> <ol> <li>Click on the failed workflow in the Actions tab</li> <li>Click on the failed job (it will have a red X icon)</li> <li>Expand the failed step to see the detailed error output</li> <li>Look for the actual error message (often at the bottom of the log)</li> </ol> <p>Common Issues and Fixes</p> <p>Ruff linting fails: <pre><code># Fix locally first\nruff check practice_level_gp_appointments/ --fix\nruff format practice_level_gp_appointments/\ngit commit -am \"fix: resolve linting issues\"\n</code></pre></p> <p>Tests fail: <pre><code># Run tests locally to debug\npytest tests/ -v\n# Fix the failing tests, then commit\n</code></pre></p> <p>Documentation build fails: <pre><code># Test docs build locally\nmkdocs build --strict\n# Fix any broken links or syntax errors\n</code></pre></p>"},{"location":"workshops/github_actions/#53-add-workflow-status-badges","title":"5.3 Add Workflow Status Badges","text":"<p>Status badges show the current state of your workflows directly in your README:</p> <ol> <li>Open your README.md file</li> <li>Add badge markdown at the top of the file:</li> </ol> <pre><code># Package Your Code Workshop\n\n![Quality Checks](https://github.com/YOUR-USERNAME/package-your-code-workshop/workflows/Quality%20Checks/badge.svg)\n![Deploy Documentation](https://github.com/YOUR-USERNAME/package-your-code-workshop/workflows/Deploy%20Documentation/badge.svg)\n\nYour workshop description here...\n</code></pre> <p>Why Use Badges?</p> <p>Status badges provide immediate visual feedback about your project's health. They show visitors whether your tests are passing and if your documentation is building successfully.</p>"},{"location":"workshops/github_actions/#best-practices-for-cicd","title":"Best Practices for CI/CD","text":""},{"location":"workshops/github_actions/#workflow-organization","title":"Workflow Organization","text":"<p>Keep It Simple</p> <p>Do: - \u2705 One purpose per workflow - separate quality checks from deployment - \u2705 Descriptive names - \"Quality Checks\", not \"CI\" - \u2705 Conditional execution - only run what's needed - \u2705 Fast feedback - fail fast on code quality issues</p> <p>Don't: - \u274c Monolithic workflows - one massive workflow that does everything - \u274c Unnecessary runs - testing docs when only code changed - \u274c Silent failures - always check workflow results</p>"},{"location":"workshops/github_actions/#security-best-practices","title":"Security Best Practices","text":"<p>Security Considerations</p> <p>Permissions: - Only grant the minimum permissions needed - Use <code>contents: read</code> by default - Only add <code>pages: write</code> for deployment workflows</p> <p>Secrets: - Never commit API keys or passwords - Use GitHub repository secrets for sensitive data - Prefer GitHub's built-in authentication when possible</p>"},{"location":"workshops/github_actions/#resource-management","title":"Resource Management","text":"<p>GitHub Actions Limits</p> <p>Public repositories get: - \u2705 Unlimited minutes for public repos - \u2705 2,000 minutes/month for private repos (free tier)</p> <p>Best practices: - Cache dependencies to speed up builds - Skip unnecessary jobs with conditions - Use matrix builds carefully (they multiply resource usage)</p>"},{"location":"workshops/github_actions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"workshops/github_actions/#common-workflow-issues","title":"Common Workflow Issues","text":"<p>Workflow Not Triggering</p> <p>Check: - File is in <code>.github/workflows/</code> directory - YAML syntax is correct (use a YAML validator) - Triggers match your intended events - Branch names are correct</p> <p>Permission Denied</p> <p>For GitHub Pages deployment: - Enable GitHub Pages in repository settings - Set source to \"GitHub Actions\"  - Check workflow permissions section</p> <p>Dependencies Install Fails</p> <p>Common fixes: - Ensure pyproject.toml is valid - Check all dependency groups exist - Verify UV installation completed successfully</p>"},{"location":"workshops/github_actions/#debugging-workflow-files","title":"Debugging Workflow Files","text":"<pre><code># Validate YAML syntax locally\npython -c \"import yaml; yaml.safe_load(open('.github/workflows/quality-checks.yml'))\"\n\n# Check file encoding (should be UTF-8)\nfile .github/workflows/quality-checks.yml\n</code></pre>"},{"location":"workshops/github_actions/#checkpoint","title":"Checkpoint","text":"<p>Before finishing this workshop, verify you can:</p> <ul> <li> Create GitHub Actions workflow files in the correct directory</li> <li> Set up automated code quality checks with Ruff and pytest</li> <li> Configure conditional documentation building</li> <li> Deploy documentation automatically to GitHub Pages</li> <li> Understand workflow status and debug failures</li> <li> Apply CI/CD best practices to your projects</li> </ul>"},{"location":"workshops/github_actions/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've set up professional CI/CD workflows for your Python project.</p> <p>Your project now automatically:</p> <ul> <li>\u2705 Tests all code changes before they're merged</li> <li>\u2705 Maintains code quality with automated linting</li> <li>\u2705 Deploys documentation when changes are made</li> <li>\u2705 Provides immediate feedback on pull requests</li> </ul> <p>Continue improving your workflows:</p> <ul> <li>Add more comprehensive tests to your test suite</li> <li>Set up notifications for workflow failures</li> <li>Create workflows for releasing new versions</li> <li>Add security scanning with tools like CodeQL</li> </ul> <p>Explore other workshop topics:</p> <ul> <li>Dependency Management - Modern Python dependency management</li> <li>Packaging with pyproject.toml - Professional Python packaging  </li> <li>Documentation with MkDocs - Create beautiful documentation</li> <li>Pre-Commit Hooks - Catch issues before they reach CI</li> </ul> Additional Resources"},{"location":"workshops/github_actions/#github-actions","title":"GitHub Actions","text":"<ul> <li>GitHub Actions Documentation - Complete official guide</li> <li>Workflow Syntax - YAML reference</li> <li>GitHub Actions Marketplace - Pre-built actions</li> <li>GitHub Actions Examples - Template workflows</li> </ul>"},{"location":"workshops/github_actions/#cicd-best-practices","title":"CI/CD Best Practices","text":"<ul> <li>CI/CD Best Practices - GitHub's recommendations</li> <li>Testing Python Applications - pytest documentation</li> <li>Code Coverage - Python coverage measurement</li> </ul>"},{"location":"workshops/github_actions/#github-pages","title":"GitHub Pages","text":"<ul> <li>GitHub Pages Documentation - Official GitHub Pages guide</li> <li>Custom Domains - Using your own domain</li> <li>MkDocs Deployment - MkDocs-specific deployment guide</li> </ul>"},{"location":"workshops/github_actions/#security","title":"Security","text":"<ul> <li>Security Hardening - Secure workflow practices</li> <li>Encrypted Secrets - Managing sensitive data</li> <li>OIDC with GitHub Actions - Advanced authentication</li> </ul>"},{"location":"workshops/mkdocs_documentation/","title":"Documentation with MkDocs: Professional Docs for Your Python Projects","text":"<p>Learn how to create professional documentation for your Python projects using MkDocs Material, generate automatic API documentation, and deploy to GitHub Pages.</p> <p>Learning Objectives</p> <ul> <li>Set up MkDocs with Material theme and NHS styling</li> <li>Understand mkdocs.yml configuration structure</li> <li>Generate automatic API documentation with mkdocstrings</li> <li>Create and organize documentation pages</li> <li>Deploy your documentation to GitHub Pages</li> </ul> <p>Why This Matters for RAP</p> <p>Professional documentation is essential for Silver RAP, which requires \"well-documented code including user guidance, explanation of code structure &amp; methodology.\" This workshop teaches you to create documentation that meets these standards automatically.</p>"},{"location":"workshops/mkdocs_documentation/#task-1-understanding-mkdocs-and-material-theme-setup","title":"Task 1: Understanding MkDocs and Material Theme Setup","text":"<p>Let's explore how MkDocs is already configured in this repository with NHS Data Science team styling.</p>"},{"location":"workshops/mkdocs_documentation/#11-examine-the-current-setup","title":"1.1 Examine the Current Setup","text":"<p>First, let's look at the mkdocs.yml configuration file:</p> <pre><code># View the MkDocs configuration\ncat mkdocs.yml\n</code></pre> <p>MkDocs Basics</p> <p>MkDocs is a fast, simple static site generator for building project documentation. Material for MkDocs is a popular theme that provides a modern, responsive design.</p>"},{"location":"workshops/mkdocs_documentation/#12-key-configuration-sections","title":"1.2 Key Configuration Sections","text":"<p>Let's understand the main parts of our mkdocs.yml:</p>"},{"location":"workshops/mkdocs_documentation/#site-information","title":"Site Information","text":"<pre><code>site_name: Package Your Code Workshop # (1)!\nsite_description: NHS Data Science Workshop - Package Your Code # (2)!\nsite_author: Joseph Wilson - NHS England - Data Science and Applied AI Team # (3)!\nsite_url: https://nhsengland.github.io/package-your-code-workshop # (4)!\n</code></pre> <ol> <li>Display name shown in browser tab and site header - keep it concise and descriptive</li> <li>Brief description for search engines and social media sharing - appears in meta tags</li> <li>Author information for attribution and contact - helps with project ownership clarity</li> <li>Full URL where the site will be deployed - enables proper linking and canonical URLs</li> </ol>"},{"location":"workshops/mkdocs_documentation/#material-theme-configuration","title":"Material Theme Configuration","text":"<pre><code>theme:\n  name: material # (1)!\n  language: en # (2)!\n  custom_dir: docs/overrides # (3)!\n  palette:\n    scheme: default # (4)!\n    primary: indigo # (5)!\n  logo: images/logo/nhs-blue-on-white.jpg # (6)!\n  favicon: images/favicon/favicon.ico # (7)!\n</code></pre> <ol> <li>Use the Material theme for MkDocs - provides modern, responsive design</li> <li>Set the site language to English for proper accessibility and SEO</li> <li>Point to custom NHS templates and styling overrides</li> <li>Use the default (light) colour scheme - can also be 'slate' for dark mode</li> <li>Set primary colour to indigo to match NHS branding guidelines</li> <li>NHS logo displayed in the site header navigation</li> <li>Custom favicon for the browser tab - uses NHS branding</li> </ol>"},{"location":"workshops/mkdocs_documentation/#navigation-structure","title":"Navigation Structure","text":"<pre><code>nav:\n  - Home: index.md # (1)!\n  - Getting Started: getting_started.md # (2)!\n  - Workshops: # (3)!\n    - workshops/index.md # (4)!\n    - Dependency Management: workshops/dependency_management.md # (5)!\n    # ... more workshops\n  - API Reference: # (6)!\n    - api_reference/index.md # (7)!\n</code></pre> <ol> <li>Main landing page - provides project overview and quick start information</li> <li>Detailed setup guide for getting the project running locally</li> <li>Workshop section with dropdown navigation for all tutorials</li> <li>Workshop index page explaining the learning path and prerequisites</li> <li>Individual workshop pages - each covers a specific packaging topic</li> <li>API Reference section for automatically generated documentation</li> <li>API index page with overview of all modules and quick examples</li> </ol> <p>NHS Styling</p> <p>This repository uses NHS branding with custom colours, logos, and styling. The <code>custom_dir: docs/overrides</code> points to NHS-specific templates and the <code>extra_css</code> section includes NHS styling.</p>"},{"location":"workshops/mkdocs_documentation/#13-test-the-current-setup","title":"1.3 Test the Current Setup","text":"<p>Let's run the documentation server to see what we have:</p> With UV (If you've done Dependency Management)With pip + venv (Basic Setup) <p>If you've completed the Dependency Management workshop:</p> <pre><code># Activate your UV environment\nsource .venv/bin/activate\n\n# Install documentation dependencies with UV\nuv sync --group docs\n\n# Start the development server\nmkdocs serve\n</code></pre> <p>Using the basic repository setup with requirements.txt:</p> <pre><code># Create and activate virtual environment (if not already done)\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install documentation dependencies from requirements.txt\npip install -r requirements.txt\n\n# Start the development server\nmkdocs serve\n</code></pre> <p>Visit <code>http://127.0.0.1:8000</code> to see the documentation site running locally.</p> <p>Using GitHub Codespaces?</p> <p>If you're running this in GitHub Codespaces, the port will be automatically forwarded. Look for a popup notification or check the Ports tab at the bottom of your VS Code interface. The forwarded URL will look like <code>https://your-codespace-name-8000.preview.app.github.dev/</code></p> <p>Port Already in Use?</p> <p>If port 8000 is busy, MkDocs will automatically try 8001, 8002, etc. Check the terminal output for the actual URL.</p>"},{"location":"workshops/mkdocs_documentation/#task-2-creating-api-documentation-with-mkdocstrings","title":"Task 2: Creating API Documentation with mkdocstrings","text":"<p>Now let's generate automatic API documentation for our Python package using mkdocstrings.</p>"},{"location":"workshops/mkdocs_documentation/#21-understand-mkdocstrings-configuration","title":"2.1 Understand mkdocstrings Configuration","text":"<p>Our mkdocs.yml already includes mkdocstrings setup:</p> <pre><code>plugins:\n  - mkdocstrings: # (1)!\n      handlers:\n        python: # (2)!\n          options:\n            docstring_style: numpy # (3)!\n            members_order: source # (4)!\n            show_source: true # (5)!\n            show_bases: true # (6)!\n</code></pre> <ol> <li>The mkdocstrings plugin automatically generates API documentation from your Python docstrings</li> <li>Use the Python handler to process Python modules and extract documentation</li> <li>Parse docstrings using NumPy format - supports Parameters, Returns, Examples sections</li> <li>Display class members and functions in the same order they appear in source code</li> <li>Include links to the actual source code on GitHub for each function/class</li> <li>Show base classes for inheritance relationships in class documentation</li> </ol> Docstring Styles <p>mkdocstrings supports multiple docstring formats:</p> <p>NumPy - Structured sections with clear parameter descriptions <pre><code>def add(x, y):\n    \"\"\"Add two numbers.\n\n    Parameters\n    ----------\n    x : int\n        First number\n    y : int\n        Second number\n\n    Returns\n    -------\n    int\n        Sum of x and y\n    \"\"\"\n</code></pre></p> <p>Google - Clean, readable format popular in modern Python <pre><code>def add(x, y):\n    \"\"\"Add two numbers.\n\n    Args:\n        x (int): First number\n        y (int): Second number\n\n    Returns:\n        int: Sum of x and y\n    \"\"\"\n</code></pre></p> <p>Sphinx - Traditional reStructuredText format <pre><code>def add(x, y):\n    \"\"\"Add two numbers.\n\n    :param x: First number\n    :type x: int\n    :param y: Second number\n    :type y: int\n    :return: Sum of x and y\n    :rtype: int\n    \"\"\"\n</code></pre></p> <p>PEP 257 - Basic Python docstring conventions <pre><code>def add(x, y):\n    \"\"\"Add two numbers and return the result.\"\"\"\n</code></pre></p> <p>mkdocstrings can automatically detect and parse mixed styles within the same project, making it flexible for teams with varying documentation preferences.</p>"},{"location":"workshops/mkdocs_documentation/#22-examine-existing-python-modules","title":"2.2 Examine Existing Python Modules","text":"<p>Let's look at the structure of our Python package:</p> <pre><code># List the Python modules in our package\nls -la practice_level_gp_appointments/\n\n# Look at an example module with docstrings\nhead -20 practice_level_gp_appointments/data_processing.py\n</code></pre>"},{"location":"workshops/mkdocs_documentation/#23-create-api-documentation-pages","title":"2.3 Create API Documentation Pages","text":"<p>Now let's create documentation pages for each module. First, examine the current API reference structure:</p> <pre><code># Check what's in the API reference directory\nls -la docs/content/api_reference/\ncat docs/content/api_reference/index.md\n</code></pre> <p>Let's create individual documentation pages for each module. We'll start with the data processing module:</p> <ol> <li>Create a new file called <code>data_processing.md</code> in the <code>docs/content/api_reference/</code> directory</li> <li>Add the following content:</li> </ol> <pre><code># Data Processing Module\n\n::: practice_level_gp_appointments.data_processing\n</code></pre> <p>mkdocstrings Syntax</p> <p>The <code>module.name</code> syntax tells mkdocstrings to automatically generate documentation for that module, including all functions, classes, and their docstrings.</p> Create the Other Module Pages <p>Now create the remaining API documentation pages following the same pattern:</p> <p>Create <code>analytics.md</code> in <code>docs/content/api_reference/</code>: <pre><code># Analytics Module\n\n::: practice_level_gp_appointments.analytics\n</code></pre></p> <p>Create <code>visualization.md</code> in <code>docs/content/api_reference/</code>: <pre><code># Visualization Module\n\n::: practice_level_gp_appointments.visualization\n</code></pre></p> <p>Create <code>output.md</code> in <code>docs/content/api_reference/</code>: <pre><code># Output Module\n\n::: practice_level_gp_appointments.output\n</code></pre></p> <p>Create <code>pipeline.md</code> in <code>docs/content/api_reference/</code>: <pre><code># Pipeline Module\n\n::: practice_level_gp_appointments.pipeline\n</code></pre></p>"},{"location":"workshops/mkdocs_documentation/#24-update-navigation","title":"2.4 Update Navigation","text":"<p>Now let's add these new pages to our navigation in mkdocs.yml:</p> <pre><code>nav:\n  # ... existing navigation ...\n  - API Reference: # (1)!\n    - api_reference/index.md # (2)!\n    - Data Processing: api_reference/data_processing.md # (3)!\n    - Analytics: api_reference/analytics.md # (4)!\n    - Visualization: api_reference/visualization.md # (5)!\n</code></pre> <ol> <li>Top-level navigation item with dropdown menu for API documentation</li> <li>Overview page explaining the API structure and providing quick examples</li> <li>Dedicated page for data processing functions (loading, cleaning, transforming)</li> <li>Dedicated page for analytical functions (statistics, summaries, calculations)</li> <li>Dedicated page for visualization functions (charts, plots, dashboards)</li> </ol> <p>Edit mkdocs.yml Carefully</p> <p>YAML is sensitive to indentation. Make sure to maintain the same indentation level as existing items in the nav section.</p>"},{"location":"workshops/mkdocs_documentation/#25-test-api-documentation","title":"2.5 Test API Documentation","text":"<p>Let's see our API documentation in action:</p> <pre><code># If mkdocs serve is still running, it should auto-reload\n# Otherwise, restart it:\nmkdocs serve\n</code></pre> <p>Navigate to the API Reference section and explore the automatically generated documentation.</p> <p>What You Should See</p> <ul> <li>Function signatures with parameter types</li> <li>Docstring content formatted nicely</li> <li>Source code links</li> <li>Class inheritance information</li> </ul>"},{"location":"workshops/mkdocs_documentation/#task-3-customizing-and-organizing-documentation","title":"Task 3: Customizing and Organizing Documentation","text":"<p>Let's improve our documentation structure and add some custom content.</p>"},{"location":"workshops/mkdocs_documentation/#31-create-a-comprehensive-api-reference-index","title":"3.1 Create a Comprehensive API Reference Index","text":"<p>Let's update the API reference index page to include package-level auto-documentation and a clean navigation table:</p> <ol> <li>Open the file <code>docs/content/api_reference/index.md</code></li> <li>Replace its contents with:</li> </ol> <pre><code># API Reference\n\n::: practice_level_gp_appointments\n\n## Module Documentation\n\n| Module | Description | Key Components |\n|--------|-------------|----------------|\n| [Data Processing](data_processing.md) | Data loading, cleaning, and transformation | DataLoadingStage, DataJoiningStage |\n| [Analytics](analytics.md) | Statistical analysis and summaries | SummarisationStage |\n| [Visualization](visualization.md) | Chart generation and plotting | Visualization functions |\n| [Output](output.md) | Data export and report generation | OutputStage |\n| [Pipeline](pipeline.md) | Pipeline orchestration and workflow | NHSPracticeAnalysisPipeline |\n</code></pre> <p>Complete Documentation</p> <p>The <code>practice_level_gp_appointments</code> directive automatically generates comprehensive documentation from the package's docstrings, imports, and version information.</p>"},{"location":"workshops/mkdocs_documentation/#32-update-navigation","title":"3.2 Update Navigation","text":"<p>Now let's add all these new pages to our navigation in mkdocs.yml. You'll need to update the API Reference section:</p> <pre><code>nav:\n  # ... existing navigation ...\n  - API Reference: # (1)!\n    - api_reference/index.md # (2)!\n    - Data Processing: api_reference/data_processing.md # (3)!\n    - Analytics: api_reference/analytics.md # (4)!\n    - Visualization: api_reference/visualization.md # (5)!\n    - Output: api_reference/output.md # (6)!\n    - Pipeline: api_reference/pipeline.md # (7)!\n</code></pre> <ol> <li>Top-level navigation item with dropdown menu for API documentation</li> <li>Overview page with package documentation and navigation table</li> <li>Dedicated page for data processing functions and classes</li> <li>Dedicated page for analytical functions and classes</li> <li>Dedicated page for visualization functions and classes</li> <li>Dedicated page for output and export functions</li> <li>Dedicated page for pipeline orchestration classes</li> </ol> <p>Edit mkdocs.yml Carefully</p> <p>YAML is sensitive to indentation. Make sure to maintain the same indentation level as existing items in the nav section.</p>"},{"location":"workshops/mkdocs_documentation/#33-test-your-api-documentation","title":"3.3 Test Your API Documentation","text":"<p>Now let's see your API documentation in action:</p> <pre><code># If mkdocs serve is still running, it should auto-reload\n# Otherwise, restart it:\nmkdocs serve\n</code></pre> <p>Visit <code>http://127.0.0.1:8000</code> and navigate to the API Reference section.</p> <p>Using GitHub Codespaces?</p> <p>If you're running this in GitHub Codespaces, use the forwarded URL from the Ports tab instead of localhost. It will look like <code>https://your-codespace-name-8000.preview.app.github.dev/</code></p> <p>What You Should See</p> <p>On the API Reference index page: - Package description and version from the <code>__init__.py</code> file - All available classes and functions imported at package level - Clean navigation table with links to each module</p> <p>On individual module pages: - Function signatures with parameter types - Docstring content formatted nicely with sections - Source code links (if configured) - Class inheritance information</p> <p>Overall navigation: - API Reference dropdown in the main navigation - Each module accessible from the dropdown menu</p>"},{"location":"workshops/mkdocs_documentation/#task-4-deploying-to-github-pages","title":"Task 4: Deploying to GitHub Pages","text":"<p>Now let's set up your documentation to be deployed to GitHub Pages using the manual method. GitHub Pages is a free static site hosting service can serve your documentation directly from your repository. In this workshop, we'll use the manual deployment method where you build locally and push to a <code>gh-pages</code> branch.</p> <p>Make sure you are on a forked repository!</p> <p>If you're following this workshop as part of the complete package-your-code workshop series, you're likely already working on your own fork. You can skip the forking step and continue with the repository you've been using.</p> <p>Find out how to fork the repository in the Getting Started guide.</p> Automatic Deployment with GitHub Actions <p>You can also set up automatic deployment using GitHub Actions, which builds and deploys your documentation automatically when you push changes. This is covered in detail in the CI/CD with GitHub Actions workshop.</p> <p>The automated approach uses workflows that run on GitHub's servers, eliminating the need to build locally and ensuring documentation stays up-to-date with every code change.</p>"},{"location":"workshops/mkdocs_documentation/#41-enable-github-pages","title":"4.1 Enable GitHub Pages","text":"<ol> <li>Go to your repository on GitHub</li> <li>Click Settings tab</li> <li>Scroll down to Pages section</li> <li>Under Source, select Deploy from a branch</li> <li>Choose gh-pages branch and / (root) folder</li> <li>Click Save</li> </ol> <p>No gh-pages Branch Yet?</p> <p>Don't worry! We'll create it in the next step. GitHub Pages will show an error until we deploy for the first time.</p>"},{"location":"workshops/mkdocs_documentation/#42-deploy-using-mkdocs","title":"4.2 Deploy Using MkDocs","text":"<p>MkDocs has a built-in deployment command for GitHub Pages:</p> With UV (If you've done Dependency Management)With pip + venv (Basic Setup) <p>If you've completed the Dependency Management workshop:</p> <pre><code># Build and deploy to GitHub Pages with UV\nuv run mkdocs gh-deploy\n\n# This command:\n# 1. Runs mkdocs through UV's environment\n# 2. Builds your documentation\n# 3. Creates/updates the gh-pages branch\n# 4. Pushes to GitHub\n</code></pre> <p>Using the basic repository setup with requirements.txt:</p> <pre><code># Build and deploy to GitHub Pages\nmkdocs gh-deploy\n\n# This command:\n# 1. Builds your documentation\n# 2. Creates/updates the gh-pages branch\n# 3. Pushes to GitHub\n</code></pre> <p>First Deployment</p> <p>After the first <code>mkdocs gh-deploy</code>, your site will be available at: <code>https://YOUR-USERNAME.github.io/package-your-code-workshop/</code></p>"},{"location":"workshops/mkdocs_documentation/#45-test-your-deployment","title":"4.5 Test Your Deployment","text":"<p>After deployment:</p> <ol> <li>Visit your GitHub Pages URL</li> <li>Test all navigation links</li> <li>Verify API documentation displays correctly</li> <li>Check that images and styling work</li> </ol> <p>Updates</p> <p>To update your documentation, simply run <code>mkdocs gh-deploy</code> again after making changes to your documentation files.</p>"},{"location":"workshops/mkdocs_documentation/#checkpoint","title":"Checkpoint","text":"<p>Before moving to the next workshop, verify you can:</p> <ul> <li> Understand mkdocs.yml configuration structure</li> <li> Run <code>mkdocs serve</code> to preview documentation locally</li> <li> Create API documentation pages using mkdocstrings</li> <li> Add new pages to the navigation structure</li> <li> Deploy documentation to GitHub Pages using <code>mkdocs gh-deploy</code></li> <li> Access your documentation at your GitHub Pages URL</li> </ul>"},{"location":"workshops/mkdocs_documentation/#next-steps","title":"Next Steps","text":"<p>Excellent work! You've created professional documentation that meets RAP standards.</p> <p>Continue your learning journey - these workshops can be done in any order:</p> <ul> <li>Dependency Management - Modern Python dependency management</li> <li>Packaging with pyproject.toml - Make your code installable and reusable</li> <li>Pre-Commit Hooks - Automate code quality checks</li> <li>CI/CD with GitHub Actions - Automate testing and deployment</li> </ul> Additional Resources"},{"location":"workshops/mkdocs_documentation/#mkdocs-and-material-theme","title":"MkDocs and Material Theme","text":"<ul> <li>MkDocs Documentation - Official MkDocs guide</li> <li>Material for MkDocs - Complete Material theme documentation</li> <li>MkDocs Configuration - mkdocs.yml reference</li> <li>Material Theme Setup - Getting started with Material</li> </ul>"},{"location":"workshops/mkdocs_documentation/#api-documentation","title":"API Documentation","text":"<ul> <li>mkdocstrings Documentation - Automatic API documentation</li> <li>mkdocstrings Python Handler - Python-specific configuration</li> <li>NumPy Docstring Guide - Docstring formatting standards</li> <li>Google Style Docstrings - Alternative docstring format</li> </ul>"},{"location":"workshops/mkdocs_documentation/#github-pages-and-deployment","title":"GitHub Pages and Deployment","text":"<ul> <li>GitHub Pages Documentation - Official GitHub Pages guide</li> <li>MkDocs Deployment - Deployment options and strategies</li> <li>GitHub Actions for MkDocs - Automated deployment options</li> </ul>"},{"location":"workshops/mkdocs_documentation/#rap-documentation-standards","title":"RAP Documentation Standards","text":"<ul> <li>RAP Documentation Requirements - Silver RAP documentation standards</li> <li>NHS Digital Documentation Style - NHS content and style guidelines</li> </ul>"},{"location":"workshops/packaging_pyproject/","title":"Packaging with pyproject.toml: Modern Python Project Configuration","text":"<p>Learn how to properly configure your Python projects using pyproject.toml, the modern standard for Python packaging and project metadata.</p> <p>Learning Objectives</p> <ul> <li>Understand the current minimal pyproject.toml configuration</li> <li>Add comprehensive project metadata and information</li> <li>Configure dynamic version management from <code>__init__.py</code></li> <li>Set up tool configurations for code quality tools</li> <li>Follow modern Python packaging standards</li> </ul> <p>Why This Matters for RAP</p> <p>Proper project configuration is essential for Gold RAP and useful for Silver RAP. The pyproject.toml file standardizes how Python projects are configured, making them more maintainable, discoverable, and professional. For Silver RAP and above, many development tools and settings can be centrally configured in pyproject.toml.</p>"},{"location":"workshops/packaging_pyproject/#task-1-understanding-the-current-pyprojecttoml","title":"Task 1: Understanding the Current pyproject.toml","text":"<p>Let's examine what we currently have in our pyproject.toml file.</p> <p>You should see:</p> <pre><code>[project] # (1)!\nname = \"package-your-code-workshop\" # (2)!\nversion = \"0.1.0\" # (3)!\n\n[tool.setuptools.packages.find] # (4)!\ninclude = [\"practice_level_gp_appointments*\"] # (5)!\n</code></pre> <ol> <li>The <code>[project]</code> section contains core project metadata defined by PEP 621</li> <li>Project name - must be unique if publishing to PyPI, should follow Python naming conventions</li> <li>Static version number - we'll configure this to be dynamic later in the workshop</li> <li>Tool-specific configuration section for setuptools (our build backend)</li> <li>Tells setuptools which packages to include when building - the <code>*</code> includes subpackages</li> </ol>"},{"location":"workshops/packaging_pyproject/#task-2-adding-comprehensive-project-metadata","title":"Task 2: Adding Comprehensive Project Metadata","text":"<p>Let's expand our project configuration with proper metadata that makes our package professional and discoverable.</p>"},{"location":"workshops/packaging_pyproject/#21-add-core-project-information","title":"2.1 Add Core Project Information","text":"<p>Open your <code>pyproject.toml</code> file and replace the <code>[project]</code> section with your own details:</p> <p>Personalizing Your Package</p> <p>Make it yours! Replace \"Your Name\" and \"your.email@nhs.net\" with your actual details. This is important for:</p> <ul> <li>Attribution - You get credit for your work alongside the original author</li> <li>Contact - People know who to reach for questions about your contributions</li> <li>Professional development - Your name appears in package metadata</li> <li>Portfolio building - Contributes to your coding portfolio</li> </ul> <pre><code>[project]\nname = \"package-your-code-workshop\"\nversion = \"0.1.0\"\ndescription = \"NHS Data Science Workshop - Learn to package your Python code professionally\" # (1)!\nreadme = \"README.md\" # (2)!\nlicense = {text = \"MIT\"} # (3)!\nrequires-python = \"&gt;=3.9\" # (4)!\nauthors = [ # (5)!\n    {name = \"Joseph Wilson\", email = \"joseph.wilson@nhs.net\"}, # (6)!\n    {name = \"Your Name\", email = \"your.email@nhs.net\"}, # (7)!\n    {name = \"NHS England Data Science Team\"},\n]\nmaintainers = [ # (8)!\n    {name = \"NHS England Data Science Team\", email = \"datascience@nhs.net\"},\n]\nkeywords = [\"nhs\", \"data-science\", \"packaging\", \"workshop\", \"gp-appointments\"] # (9)!\n</code></pre> <ol> <li>Clear, concise description of what the project does</li> <li>Points to the README file for detailed project information</li> <li>License specification - references the MIT license in our LICENSE file</li> <li>Minimum Python version required - important for compatibility</li> <li>Authors who created the project - can include name and/or email</li> <li>The very good looking, talented, and, most of all, humble creator of this workshop</li> <li>Add your own name and email here - you're contributing to this project!</li> <li>Current maintainers responsible for ongoing development</li> <li>Keywords help with discoverability in package indexes</li> </ol>"},{"location":"workshops/packaging_pyproject/#22-add-project-urls-and-classifiers","title":"2.2 Add Project URLs and Classifiers","text":"<p>Continue adding to your <code>[project]</code> section:</p> <p>Customize Your Project URLs</p> <p>If you've completed the MkDocs Documentation workshop and set up GitHub Pages, update these URLs to point to your own repository and documentation:</p> <ul> <li>Homepage &amp; Documentation: <code>https://yourusername.github.io/package-your-code-workshop</code></li> <li>Repository: <code>https://github.com/yourusername/package-your-code-workshop</code></li> <li>Bug Tracker: <code>https://github.com/yourusername/package-your-code-workshop/issues</code></li> </ul> <p>This makes your package truly yours and showcases your own documentation site!</p> <pre><code>[project.urls] # (1)!\nHomepage = \"https://nhsengland.github.io/package-your-code-workshop\"\nDocumentation = \"https://nhsengland.github.io/package-your-code-workshop\"\nRepository = \"https://github.com/nhsengland/package-your-code-workshop\"\n\"Bug Tracker\" = \"https://github.com/nhsengland/package-your-code-workshop/issues\"\n\nclassifiers = [ # (2)!\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Healthcare Industry\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n]\n</code></pre> <ol> <li>URLs section provides important project links for users and tools - customize these!</li> <li>Classifiers categorize your project in package indexes like PyPI</li> </ol> <p>PyPI Classifiers</p> <p>PyPI Classifiers are standardized tags that help categorize packages. They improve discoverability and help users understand your project's purpose and compatibility.</p>"},{"location":"workshops/packaging_pyproject/#23-test-your-configuration","title":"2.3 Test Your Configuration","text":"<p>Let's verify our configuration is valid:</p> With UV (If you've done Dependency Management)With pip + venv (Basic Setup) <pre><code># Use traditional build tool for dry-run testing\nuv run python -m pip install build\nuv run python -m build --wheel --no-isolation --dry-run\n</code></pre> <p>Why not <code>uv build</code>?</p> <p>UV's <code>uv build</code> command doesn't have a <code>--dry-run</code> option, so we use the traditional build tool to test our configuration without creating files.</p> <pre><code># Check if pyproject.toml is valid and test build\npython -m pip install build\npython -m build --wheel --no-isolation --dry-run\n</code></pre> <p>What You Should See</p> <ul> <li>No syntax errors in the TOML format</li> <li>Build process completes successfully</li> <li>All metadata is properly recognized</li> </ul>"},{"location":"workshops/packaging_pyproject/#task-3-dynamic-version-management","title":"Task 3: Dynamic Version Management","text":"<p>Instead of manually updating version numbers in multiple places, let's configure dynamic versioning from our <code>__init__.py</code> file.</p>"},{"location":"workshops/packaging_pyproject/#31-examine-current-version-setup","title":"3.1 Examine Current Version Setup","text":"<p>First, let's see how version is currently defined:</p> <pre><code># Check the version in __init__.py\ngrep -n \"__version__\" practice_level_gp_appointments/__init__.py\n</code></pre>"},{"location":"workshops/packaging_pyproject/#32-configure-dynamic-versioning","title":"3.2 Configure Dynamic Versioning","text":"<p>Update your <code>[project]</code> section to use dynamic versioning:</p> <pre><code>[project]\nname = \"package-your-code-workshop\"\ndynamic = [\"version\"] # (1)!\ndescription = \"NHS Data Science Workshop - Learn to package your Python code professionally\"\n# ... rest of your project configuration\n</code></pre> <ol> <li>Tells build tools that version should be determined dynamically</li> </ol> <p>Then add the setuptools configuration to read from <code>__init__.py</code>:</p> <pre><code>[tool.setuptools.dynamic] # (1)!\nversion = {attr = \"practice_level_gp_appointments.__version__\"} # (2)!\n</code></pre> <ol> <li>Setuptools-specific configuration for dynamic fields</li> <li>Points to the <code>__version__</code> variable in our package's <code>__init__.py</code></li> </ol>"},{"location":"workshops/packaging_pyproject/#33-test-dynamic-versioning","title":"3.3 Test Dynamic Versioning","text":"<p>Let's verify the dynamic versioning works:</p> With UV (If you've done Dependency Management)With pip + venv (Basic Setup) <pre><code># Test the build again to ensure version is read correctly\nuv run python -m build --wheel --no-isolation --dry-run\n</code></pre> <pre><code># Test the build again to ensure version is read correctly\npython -m build --wheel --no-isolation --dry-run\n</code></pre> <p>Version Management Benefits</p> <ul> <li>Single source of truth - version only defined in <code>__init__.py</code></li> <li>Automatic consistency - build tools read the same version</li> <li>Easier releases - update version in one place</li> </ul> Alternative Versioning Approaches <p>Other dynamic versioning options include:</p> <p>From Git tags using <code>setuptools-scm</code>: <pre><code># In pyproject.toml\n[project]\ndynamic = [\"version\"]\n\n[tool.setuptools_scm]\n# Version from git tags (e.g., v1.0.0)\n</code></pre> <pre><code># Quick setup\npip install setuptools-scm\ngit tag v0.1.0  # Create your first tag\n</code></pre></p> <p>From a VERSION file: <pre><code># In pyproject.toml\n[tool.setuptools.dynamic]\nversion = {file = \"VERSION\"}\n</code></pre> <pre><code># Quick setup\necho \"0.1.0\" &gt; VERSION\n</code></pre></p> <p>From environment variables: <pre><code># In pyproject.toml\n[tool.setuptools.dynamic]\nversion = {attr = \"your_package._version.__version__\"}\n</code></pre> <pre><code># In your_package/_version.py\nimport os\n__version__ = os.getenv(\"PACKAGE_VERSION\", \"0.1.0-dev\")\n</code></pre></p>"},{"location":"workshops/packaging_pyproject/#task-4-configuring-development-tools","title":"Task 4: Configuring Development Tools","text":"<p>Let's configure code quality tools in our pyproject.toml to maintain consistent coding standards.</p>"},{"location":"workshops/packaging_pyproject/#41-configure-ruff-linter-and-formatter","title":"4.1 Configure Ruff (Linter and Formatter)","text":"<p>Add Ruff configuration to your pyproject.toml:</p> <pre><code>[tool.ruff] # (1)!\nline-length = 88 # (2)!\ntarget-version = \"py39\" # (3)!\n\n[tool.ruff.lint] # (4)!\nselect = [ # (5)!\n    \"E\",  # pycodestyle errors\n    \"W\",  # pycodestyle warnings\n    \"F\",  # Pyflakes\n    \"I\",  # isort\n    \"B\",  # flake8-bugbear\n    \"C4\", # flake8-comprehensions\n    \"UP\", # pyupgrade\n]\nignore = [ # (6)!\n    \"E501\", # line too long (handled by formatter)\n]\n\n[tool.ruff.lint.isort] # (7)!\nknown-first-party = [\"practice_level_gp_appointments\"]\n</code></pre> <ol> <li>Main Ruff configuration section</li> <li>Maximum line length (matches Black default)</li> <li>Target Python version for rule selection</li> <li>Linting-specific configuration</li> <li>Enable specific rule categories for comprehensive checking</li> <li>Disable rules that conflict with the formatter</li> <li>Configure import sorting with our package as first-party</li> </ol> <p>Now remove the old configuration file to avoid conflicts:</p> <pre><code># Remove the old ruff.toml file\nrm ruff.toml\n</code></pre> <p>Configuration Migration</p> <p>After adding Ruff configuration to pyproject.toml, delete the old ruff.toml file to prevent configuration conflicts. Ruff reads configuration in a specific order, and having both files can lead to unexpected behavior.</p>"},{"location":"workshops/packaging_pyproject/#42-test-ruff-configuration","title":"4.2 Test Ruff Configuration","text":"<p>Our repository already has Ruff installed and configured. Let's test our new pyproject.toml configuration:</p> With UV (If you've done Dependency Management)With pip + venv (Basic Setup) <pre><code># Run linting with our new pyproject.toml config\nuv run ruff check practice_level_gp_appointments/\n\n# Run formatting (shows what would change)\nuv run ruff format --diff practice_level_gp_appointments/\n</code></pre> <pre><code># Run linting with our new pyproject.toml config\nruff check practice_level_gp_appointments/\n\n# Run formatting (shows what would change)\nruff format --diff practice_level_gp_appointments/\n</code></pre> <p>Centralizing Configuration</p> <p>By moving Ruff configuration to pyproject.toml, we're centralizing all our project settings in one place. Ruff will automatically read the configuration from pyproject.toml.</p> <p>Ruff Benefits</p> <p>Ruff is extremely fast and combines multiple tools: - Linter (replaces flake8, isort, pyupgrade, and more) - Formatter (replaces Black) - Single tool instead of managing multiple dependencies</p>"},{"location":"workshops/packaging_pyproject/#43-additional-tool-configurations","title":"4.3 Additional Tool Configurations","text":"Other Common Tool Configurations <p>You can configure other development tools in pyproject.toml:</p> <p>Pytest Configuration: <pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\", \"*_test.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\naddopts = \"-v --tb=short --strict-markers\"\nmarkers = [\n    \"slow: marks tests as slow\",\n    \"integration: marks tests as integration tests\",\n]\n</code></pre></p> <p>Black Formatter Configuration: <pre><code>[tool.black]\nline-length = 88\ntarget-version = ['py39']\ninclude = '\\.pyi?$'\nexclude = '''\n/(\n    \\.git\n  | \\.mypy_cache\n  | \\.tox\n  | \\.venv\n  | _build\n  | buck-out\n  | build\n  | dist\n)/\n'''\n</code></pre></p> <p>Coverage Configuration: <pre><code>[tool.coverage.run]\nsource = [\"practice_level_gp_appointments\"]\nomit = [\"*/tests/*\", \"*/test_*\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n]\n</code></pre></p> <p>MyPy Type Checking: <pre><code>[tool.mypy]\npython_version = \"3.9\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\n</code></pre></p>"},{"location":"workshops/packaging_pyproject/#44-run-all-quality-checks","title":"4.4 Run All Quality Checks","text":"<p>Let's test our complete setup:</p> With UV (If you've done Dependency Management)With pip + venv (Basic Setup) <pre><code># Run ruff linting\nuv run ruff check practice_level_gp_appointments/\n\n# Run ruff formatting\nuv run ruff format practice_level_gp_appointments/\n\n# Test the build process\nuv run python -m build --wheel --no-isolation --dry-run\n</code></pre> <pre><code># Run ruff linting\nruff check practice_level_gp_appointments/\n\n# Run ruff formatting\nruff format practice_level_gp_appointments/\n\n# Test the build process\npython -m build --wheel --no-isolation --dry-run\n</code></pre> <p>Quality Assurance Complete</p> <p>Your pyproject.toml now provides: - Professional metadata for package discovery - Dynamic versioning for easier maintenance - Tool configuration for consistent code quality</p>"},{"location":"workshops/packaging_pyproject/#task-5-using-your-packaged-code-in-other-projects","title":"Task 5: Using Your Packaged Code in Other Projects","text":"<p>Now that we've properly configured our package, let's see how to use it in other projects - just like we use <code>nhs_herbot</code> and <code>oops_its_a_pipeline</code> in our dependencies.</p>"},{"location":"workshops/packaging_pyproject/#51-understanding-git-based-dependencies","title":"5.1 Understanding Git-Based Dependencies","text":"<p>In our dependency management workshop, we saw examples like:</p> <pre><code>dependencies = [\n    \"pandas&gt;=2.1.0\",\n    \"oops_its_a_pipeline@git+https://github.com/nhsengland/oops-its-a-pipeline.git\",\n    \"nhs_herbot@git+https://github.com/nhsengland/nhs_herbot.git\",\n]\n</code></pre> <p>These are git-based dependencies - packages installed directly from GitHub repositories. Now that our project is properly packaged, we can use it the same way!</p>"},{"location":"workshops/packaging_pyproject/#52-make-your-code-available","title":"5.2 Make Your Code Available","text":"<p>First, ensure your code is available on GitHub (you should already have this from previous workshops):</p> <pre><code># Check your git status\ngit status\n\n# If you have uncommitted changes, commit them\ngit add .\ngit commit -m \"feat: complete pyproject.toml configuration with metadata and tools\"\n\n# Push to your repository (if you haven't already)\ngit push origin main\n</code></pre>"},{"location":"workshops/packaging_pyproject/#53-create-a-new-test-project","title":"5.3 Create a New Test Project","text":"<p>Let's create a simple test project to demonstrate importing your packaged code:</p> <pre><code># Move to a different directory (outside your current project)\ncd ..\n\n# Create a new test project directory\nmkdir test-import-project\ncd test-import-project\n</code></pre> <p>Now create a <code>pyproject.toml</code> file for your test project. Copy and paste this content into a new <code>pyproject.toml</code> file:</p> <pre><code>[project]\nname = \"test-import-project\"\nversion = \"0.1.0\"\ndescription = \"Testing import of our packaged GP appointments code\"\ndependencies = [\n    \"pandas&gt;=2.0.0\",\n    # We'll add our package dependency here\n]\n</code></pre>"},{"location":"workshops/packaging_pyproject/#54-add-your-package-as-a-dependency","title":"5.4 Add Your Package as a Dependency","text":"<p>Now let's add your properly packaged code as a git dependency. Update your <code>pyproject.toml</code> file with your repository details:</p> <p>Update with Your Repository</p> <p>Replace <code>YOUR-USERNAME</code> with your actual GitHub username in the configuration below!</p> <pre><code>[project]\nname = \"test-import-project\"\nversion = \"0.1.0\"\ndescription = \"Testing import of our packaged GP appointments code\"\ndependencies = [\n    \"pandas&gt;=2.0.0\",\n    \"package-your-code-workshop@git+https://github.com/YOUR-USERNAME/package-your-code-workshop.git\",\n]\n</code></pre>"},{"location":"workshops/packaging_pyproject/#55-install-and-test-your-package","title":"5.5 Install and Test Your Package","text":"<p>Now let's install your package and test that we can import it:</p> With UVWith pip + venv <pre><code># Create virtual environment and install dependencies\nuv venv\nsource .venv/bin/activate\nuv sync\n\n# Test importing your package\nuv run python -c \"import practice_level_gp_appointments; print('Success! Imported your package')\"\n\n# Test accessing your package's functions\nuv run python -c \"\nfrom practice_level_gp_appointments.analytics import SummarisationStage\nprint('Successfully imported SummarisationStage class!')\nprint(SummarisationStage.__doc__)\n\"\n</code></pre> <pre><code># Create virtual environment and install dependencies\npython -m venv .venv\nsource .venv/bin/activate\npip install -e .\n\n# Test importing your package\npython -c \"import practice_level_gp_appointments; print('Success! Imported your package')\"\n\n# Test accessing your package's functions\npython -c \"\nfrom practice_level_gp_appointments.analytics import SummarisationStage\nprint('Successfully imported SummarisationStage class!')\nprint(SummarisationStage.__doc__)\n\"\n</code></pre> <p>Import Test Complete</p> <p>If the commands above run without errors, your package is successfully configured and can be imported into other projects!</p>"},{"location":"workshops/packaging_pyproject/#56-advanced-using-specific-versions","title":"5.6 Advanced: Using Specific Versions","text":"<p>You can also specify particular versions, branches, or commits:</p> <pre><code># Specific branch\ndependencies = [\n    \"package-your-code-workshop@git+https://github.com/YOUR-USERNAME/package-your-code-workshop.git@main\",\n]\n\n# Specific tag/version\ndependencies = [\n    \"package-your-code-workshop@git+https://github.com/YOUR-USERNAME/package-your-code-workshop.git@v1.0.0\",\n]\n\n# Specific commit\ndependencies = [\n    \"package-your-code-workshop@git+https://github.com/YOUR-USERNAME/package-your-code-workshop.git@abc1234\",\n]\n</code></pre>"},{"location":"workshops/packaging_pyproject/#57-real-world-example-team-collaboration","title":"5.7 Real-World Example: Team Collaboration","text":"<p>This is exactly how teams share code within organizations:</p> <p>NHS Data Science Team Workflow</p> <p>Team Member A creates a useful data processing package: <pre><code># In their pyproject.toml\n[project]\nname = \"nhs-data-utilities\"\ndependencies = [\"pandas\", \"numpy\"]\n</code></pre></p> <p>Team Member B uses it in their analysis project: <pre><code># In their analysis project\n[project]\nname = \"mortality-trends-analysis\"\ndependencies = [\n    \"pandas&gt;=2.0.0\",\n    \"matplotlib&gt;=3.7.0\",\n    \"nhs-data-utilities@git+https://github.com/nhsengland/nhs-data-utilities.git\",\n]\n</code></pre></p> <p>Benefits: - \u2705 Reusable code - No copy-pasting between projects - \u2705 Version control - Track which version of utilities you're using - \u2705 Easy updates - Update the git reference to get new features - \u2705 Team standards - Everyone uses the same tested, documented code</p>"},{"location":"workshops/packaging_pyproject/#58-best-practices-for-git-dependencies","title":"5.8 Best Practices for Git Dependencies","text":"<p>Production Best Practices</p> <p>DO:</p> <ul> <li>\u2705 Use specific tags/versions in production: <code>@v1.2.0</code></li> <li>\u2705 Document which projects depend on your package</li> <li>\u2705 Use semantic versioning for your releases</li> <li>\u2705 Test your package in isolation before tagging releases</li> </ul> <p>DON'T:</p> <ul> <li>\u274c Point to <code>@main</code> in production (versions can change unexpectedly)</li> <li>\u274c Make breaking changes without version bumps</li> <li>\u274c Forget to update documentation when changing interfaces</li> </ul>"},{"location":"workshops/packaging_pyproject/#59-integration-with-pypi-optional","title":"5.9 Integration with PyPI (Optional)","text":"<p>For public packages, you can also publish to PyPI:</p> <pre><code># Build your package\npython -m build\n\n# Upload to PyPI (requires account and API token)\npython -m twine upload dist/*\n</code></pre> <p>Then others can install simply with: <pre><code>pip install package-your-code-workshop\n</code></pre></p> <p>PyPI Publication</p> <p>Only publish to PyPI if your package is intended for public use. For internal NHS/organizational use, git dependencies are often more appropriate.</p>"},{"location":"workshops/packaging_pyproject/#checkpoint","title":"Checkpoint","text":"<p>Before moving to the next workshop, verify you can:</p> <ul> <li> Understand the structure and purpose of pyproject.toml</li> <li> Add comprehensive project metadata including authors, description, and classifiers</li> <li> Configure dynamic version management from <code>__init__.py</code></li> <li> Set up and run code quality tools like Ruff</li> <li> Build your package successfully with proper metadata</li> <li> Use your packaged code as a dependency in other projects</li> </ul>"},{"location":"workshops/packaging_pyproject/#next-steps","title":"Next Steps","text":"<p>Excellent work! You've configured a professional Python project that follows modern standards.</p> <p>Continue your learning journey - these workshops can be done in any order:</p> <ul> <li>Dependency Management - Modern Python dependency management with UV</li> <li>Documentation with MkDocs - Professional documentation and API reference</li> <li>Pre-Commit Hooks - Automate code quality checks</li> <li>CI/CD with GitHub Actions - Automate testing and deployment</li> </ul> Additional Resources"},{"location":"workshops/packaging_pyproject/#pyprojecttoml-and-packaging","title":"pyproject.toml and Packaging","text":"<ul> <li>PEP 518 - pyproject.toml - Original specification</li> <li>PEP 621 - Project Metadata - Project metadata in pyproject.toml</li> <li>Python Packaging User Guide - Comprehensive packaging documentation</li> <li>PyPI Classifiers - Complete list of package classifiers</li> </ul>"},{"location":"workshops/packaging_pyproject/#code-quality-tools","title":"Code Quality Tools","text":"<ul> <li>Ruff Documentation - Fast Python linter and formatter</li> <li>Black Documentation - Python code formatter</li> <li>pytest Documentation - Testing framework</li> <li>MyPy Documentation - Static type checker</li> </ul>"},{"location":"workshops/packaging_pyproject/#build-tools-and-standards","title":"Build Tools and Standards","text":"<ul> <li>build Documentation - Python package build frontend</li> <li>setuptools Documentation - Python package build backend</li> <li>Wheel Format - Built distribution format</li> <li>TOML Specification - Configuration file format</li> </ul>"},{"location":"workshops/packaging_pyproject/#nhs-and-rap-standards","title":"NHS and RAP Standards","text":"<ul> <li>RAP Community of Practice - NHS RAP standards and guidance</li> </ul>"},{"location":"workshops/precommit_hooks/","title":"Pre-Commit Hooks","text":"<p>Workshop Coming Soon</p> <p>This workshop is currently under development and will be available soon. Stay tuned!</p> <p>Bonus Workshop - Self-Paced</p> <p>This is an optional, self-paced workshop. You can complete it at your own speed and refer back to it as needed.</p> <p>Learn how to automate code quality checks with pre-commit hooks.</p>"}]}